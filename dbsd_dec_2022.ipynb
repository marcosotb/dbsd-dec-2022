{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UqbP_AqsemqK"
      },
      "source": [
        "# Dreambooth Stable Diffusion - Winter 2022 Edition \n",
        "This Colab is based on Shivam Shrirao's repository and has been modified to use dependencies from late 2022 but with diffusion from revision `fbdf0a17055ffa34679cb34d986fabc1296d0785` (2023-03-02).\n",
        "\n",
        "If you prefer to use a similar layout used in the original colab, you can use [dbsd_dec_2022_original.ipynb](https://colab.research.google.com/github/yushan777/dbsd-dec-2022/blob/main/dbsd_dec_2022_original.ipynb)\n",
        "\n",
        "___\n",
        "https://github.com/yushan777/dbsd-dec-2022\n",
        "\n",
        "https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth\n",
        "\n",
        "\n",
        "Join the Dreambooth Discord!\n",
        "https://discord.gg/wNNs2JNF7G \n",
        "___\n",
        "#### Instructions:\n",
        "#### Run each cell in order. Read instructions or notes within each cell.  Last few cells are not essential - mostly tools used during testing. But have left them since they may be useful.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XU7NuMAA2drw"
      },
      "outputs": [],
      "source": [
        "#@title 0. Check type of GPU and VRAM available.\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aLWXPZqjsZVV"
      },
      "outputs": [],
      "source": [
        "#@title 1. [Google Drive & Build Environment](https://github.com/yushan777/dbsd-dec-2022/blob/main/docs/dbsd-dec-2022-readme.md#cell-2-build-environment)\n",
        "\n",
        "#@markdown > This may take a couple of minutes.....\n",
        "#@markdown >\n",
        "#@markdown > Mount Google Drive : You will be asked for permission.\\ \n",
        "\n",
        "from google.colab import drive\n",
        "from os import path\n",
        "\n",
        "google_drive_dir = '/content/drive'\n",
        "\n",
        "if path.exists(google_drive_dir)==False: \n",
        "  drive.mount(google_drive_dir)\n",
        "  print(f'Google Drive mounted to {google_drive_dir}')\n",
        "else: \n",
        "  print(f'Google Drive already mounted at {google_drive_dir}')\n",
        "\n",
        "# =================================================================================\n",
        "# =================================================================================\n",
        "\n",
        "# uninstall existing pytorch to clean things up a bit, prob unncessary but doing it anyway\n",
        "#print(\"Uninstalling existing Pytorch...\")\n",
        "#%pip -q uninstall torch torchtext torchaudio torchvision --y\n",
        "%pip -q uninstall torchtext --y\n",
        "\n",
        "print(\"Installing ShivamShrirao/Diffusers...\")\n",
        "# tested on commit fbdf0a17055ffa34679cb34d986fabc1296d0785 2023-03-02\n",
        "%pip -q install git+https://github.com/ShivamShrirao/diffusers.git@fbdf0a17055ffa34679cb34d986fabc1296d0785\n",
        "\n",
        "# get train_dreambooth.py from same revision \n",
        "!wget https://github.com/ShivamShrirao/diffusers/raw/fbdf0a17055ffa34679cb34d986fabc1296d0785/examples/dreambooth/train_dreambooth.py\n",
        "\n",
        "# for scripts we want the latest ones so that safetensors are supported\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/fbdf0a17055ffa34679cb34d986fabc1296d0785/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/fbdf0a17055ffa34679cb34d986fabc1296d0785/scripts/convert_original_stable_diffusion_to_diffusers.py\n",
        "\n",
        "# install requisite packages\n",
        "%pip install -q torch==1.12.1+cu116 torchvision==0.13.1+cu116 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "%pip install -q -U --pre triton==2.0.0.dev20221030\n",
        "%pip install omegaconf==2.3.0 # required for orig-diffusers conversion script\n",
        "%pip install pytorch-lightning==1.8.5 # required for orig-diffusers conversion script\n",
        "%pip install -q accelerate==0.12.0 transformers==4.26.0 ftfy==6.1.1 bitsandbytes==0.35.0 gradio natsort safetensors\n",
        "%pip install -q xformers==0.0.13\n",
        "%pip install -q gdown\n",
        "#%pip install -q https://github.com/yushan777/xformers-wheels/releases/download/xformers-0.015.dev0-py38/xformers-0.0.15.dev0-cp38-cp38-linux_x86_64.whl\n",
        "\n",
        "# =====================================================================================================================\n",
        "# remove instances of param 'keep_fp32_wrapper=True' from file 'train_dreambooth.py'\n",
        "import fileinput\n",
        "filename = 'train_dreambooth.py'\n",
        "with fileinput.FileInput(filename, inplace=True, backup='~bak') as file:\n",
        "    for line in file:\n",
        "        print(line.replace(', keep_fp32_wrapper=True', ''), end='')\n",
        "\n",
        "\n",
        "\n",
        "#@markdown ___\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Rxg0y5MBudmd"
      },
      "outputs": [],
      "source": [
        "#@title 2. [Token Word, Class Word & Class Prompt](https://github.com/yushan777/dbsd-dec-2022/blob/main/docs/dbsd-dec-2022-readme.md#cell-3-token-word-class-word--class-prompt)\n",
        "\n",
        "# TOKEN is a unique identifier linked to the subject that you are training\n",
        "TOKEN_WORD = \"zwx\" #@param {type:\"string\"}\n",
        "word_list = TOKEN_WORD.split()\n",
        "if len(word_list) == 0:\n",
        "    TOKEN_WORD = \"zwx\"\n",
        "elif len(word_list) > 1:\n",
        "    print(\"too many words in TOKEN_WORD - using only first word\")\n",
        "    TOKEN_WORD = word_list[0]\n",
        "\n",
        "# CLASS is a coarse class descriptor of the subject (e.g. person, man, woman, cat, dog, watch, etc.).\n",
        "CLASS_WORD = \"person\" #@param {type:\"string\"}\n",
        "word_list = CLASS_WORD.split()\n",
        "if len(word_list) == 0:\n",
        "    CLASS_WORD = \"person\"\n",
        "elif len(word_list) > 1:\n",
        "    print(\"too many words in CLASS_WORD - using only first word\")\n",
        "    CLASS_WORD = word_list[0]\n",
        "\n",
        "# CLASS is a coarse class descriptor of the subject (e.g. person, man, woman, cat, dog, watch, etc.).\n",
        "CLASS_PROMPT = \"person\" #@param {type:\"string\"}\n",
        "if len(CLASS_PROMPT) == 0:\n",
        "  CLASS_PROMPT = \"person\"\n",
        "\n",
        "#@markdown > Example Token words : `skf, lun, whoo, olis...` \\\n",
        "#@markdown > Example class words : `person, man, woman, dog, cat, car` \\\n",
        "#@markdown > \n",
        "#@markdown > Token and Class word are used together to represent your subject being trained. \\\n",
        "#@markdown > Example: `photo of [zwf] [person] sitting in a cafe.` \\\n",
        "#@markdown > \n",
        "#@markdown > Class Prompt is used for generating class images used for regularization. It can be same as Class Word or more descriptive such as, `photo of a person`.  If you are providing your own class images, this will be ignored.\n",
        "\n",
        "#@markdown ___\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "h7LJxaBO3q2z"
      },
      "outputs": [],
      "source": [
        "#@title 3. [Download / Convert Model & Set Model Path](https://github.com/yushan777/dbsd-dec-2022/blob/main/docs/dbsd-dec-2022-readme.md#cell-3-download--convert-model--set-model-path)\n",
        "\n",
        "# =========================================================================================\n",
        "# HUGGING FACE DIFFUSERS\n",
        "# =========================================================================================\n",
        "MODEL_NAME = ''\n",
        "continue_diff_converstion = False\n",
        "\n",
        "#@markdown Name or Path of the diffusers model to be trained on. (this can be a HuggingFace repo address or a local directory)\n",
        "HUGGINGFACE_MODEL_PATH = \"runwayml/stable-diffusion-v1-5\" #@param {type:\"string\"}\n",
        "#@markdown Use this field to directly download a checkpoint/safetensor model which will be converted. \\\n",
        "CKPT_SAFETENSOR_URL = '' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown > #### **If both fields are filled, then the ckpt/safetensor model will be used**\n",
        "\n",
        "# if both empty, do nothing\n",
        "# if just HF path filled, set model name\n",
        "# if both filled, process ckpt and convert to diffusers\n",
        "# if only ckpt filled, process ckpt and convert to diffusers\n",
        "\n",
        "if len(HUGGINGFACE_MODEL_PATH) > 0 and len(CKPT_SAFETENSOR_URL) == 0:  #only HF filled\n",
        "  MODEL_NAME = HUGGINGFACE_MODEL_PATH\n",
        "elif len(HUGGINGFACE_MODEL_PATH) > 0 and len(CKPT_SAFETENSOR_URL) > 0: #both filled, use last one\n",
        "  continue_diff_converstion = True\n",
        "elif len(HUGGINGFACE_MODEL_PATH) == 0 and len(CKPT_SAFETENSOR_URL) > 0: #only ckpt filled\n",
        "  continue_diff_converstion = True\n",
        "\n",
        "# =========================================================================================\n",
        "# CHECKPOINTS / SAFETENSORS\n",
        "# =========================================================================================\n",
        "if continue_diff_converstion:\n",
        "  import os\n",
        "  from glob import glob\n",
        "  from natsort import natsorted\n",
        "\n",
        "  googlelink_prefix = 'https://drive.google.com/file/d/'\n",
        "  googlelink_suffix = '/view?usp=share_link'\n",
        "\n",
        "  download_dir = '/content/downloads'\n",
        "\n",
        "  if path.exists(download_dir)==False:\n",
        "    os.mkdir(download_dir)\n",
        "\n",
        "  # check if CKPT_SAFETENSOR_url is a google drive share link\n",
        "  if CKPT_SAFETENSOR_URL.startswith(googlelink_prefix):\n",
        "    share_id = CKPT_SAFETENSOR_URL\n",
        "    print(\"is valid google share link\")\n",
        "    share_id = share_id.replace(googlelink_prefix, '')\n",
        "    share_id = share_id.replace(googlelink_suffix, '')\n",
        "    CKPT_SAFETENSOR_URL = f'https://drive.google.com/uc?id={share_id}'\n",
        "    # gdown sucks on colab and fails for large files, we will have to use wget\n",
        "    # see https://bcrf.biochem.wisc.edu/2021/02/05/download-google-drive-files-using-wget/\n",
        "    !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=$share_id' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=$share_id\" --content-disposition -P \"$download_dir\" && rm -rf /tmp/cookies.txt\n",
        "    # move file to downloads folder\n",
        "  else:\n",
        "    #download model file into download dir\n",
        "    !wget \"$CKPT_SAFETENSOR_URL\" --content-disposition -P \"$download_dir\"\n",
        "\n",
        "  # get the filename\n",
        "  search_pattern = f'{download_dir}/*'\n",
        "  file_list = natsorted(glob(f'{search_pattern}', recursive=False))\n",
        "  # if file_list is not empty...\n",
        "  if len(file_list)==0:\n",
        "    print(\"Error: No files downloaded.\")\n",
        "  else:  \n",
        "    # get last file (should be the only file)\n",
        "    file_path = file_list[-1]  \n",
        "\n",
        "    if file_path.endswith('.ckpt') or file_path.endswith('.safetensors'):    \n",
        "      # filename only\n",
        "      filename = os.path.basename(os.path.normpath(file_path))\n",
        "      # save filename without extension\n",
        "      filename_no_ext = os.path.splitext(filename)[0]\n",
        "      #print(filename_no_ext)\n",
        "      # if orig. filename ext. is safetensors then set parameter flag\n",
        "\n",
        "      from_safetensors = \"\"\n",
        "      if filename.endswith('safetensors'):\n",
        "        from_safetensors = \"--from_safetensors\"\n",
        "\n",
        "      # =========================================================================================\n",
        "      # DIFFUSERS DIRECTORY\n",
        "      # =========================================================================================\n",
        "      DIFFUSERS_DIR = \"\"\n",
        "\n",
        "      if len(DIFFUSERS_DIR)==0:\n",
        "        DIFFUSERS_DIR = f'/content/diffusers-{filename_no_ext}'\n",
        "\n",
        "      print(\"converting to diffusers... \" + DIFFUSERS_DIR)\n",
        "\n",
        "\n",
        "      # convert to diffusers \n",
        "      !python convert_original_stable_diffusion_to_diffusers.py --checkpoint_path \"$file_path\" --dump_path \"$DIFFUSERS_DIR\" $from_safetensors\n",
        "\n",
        "      # set the model name/path\n",
        "      MODEL_NAME=DIFFUSERS_DIR\n",
        "\n",
        "      print(\"Model converted to diffusers : \" + f'{DIFFUSERS_DIR}')\n",
        "    else:\n",
        "      print(\"Error : File downloaded is not a ckpt or safetensors model file.\")\n",
        "\n",
        "#@markdown > Example HF paths: \\\n",
        "#@markdown > `runwayml/stable-diffusion-v1-5` (HuggingFace)<br>\n",
        "#@markdown > `drive/MyDrive/diffusers-models/sd15` (local) \\\n",
        "#@markdown >local path can be relative as shown above or absolute as shown in the field.  \\\n",
        "#@markdown >`/content` is this colab's root \\ \n",
        "\n",
        "\n",
        "#@markdown > Example URLs:<br>\n",
        "#@markdown > `https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt` \\\n",
        "#@markdown >`https://civitai.com/api/download/models/6987?type=Model&format=PickleTensor` \\\n",
        "#@markdown >`https://civitai.com/api/download/models/6987?type=Model&format=SafeTensor` \\\n",
        "#@markdown >`https://drive.google.com/file/d/1JEZCyW36ziz9Fn482MUG8T0_2P4FGG-/view?usp=share_link` _(google drive share link)_ \\\n",
        "#@markdown >\n",
        "\n",
        "#@markdown ___\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-APTAioh6o5A"
      },
      "outputs": [],
      "source": [
        "#@title 4. [Instance, Class, Output Directory, Concepts List Settings](https://github.com/yushan777/dbsd-dec-2022/blob/main/docs/dbsd-dec-2022-readme.md#cell-4-instance-class-output-directory-concepts-list-settings)\n",
        "\n",
        "# =========================================================================================\n",
        "# INSTANCE IMAGE DIRECTORY \n",
        "# =========================================================================================\n",
        "INSTANCE_DIR = 'training_images/zwx' #@param {type:\"string\"}\n",
        "\n",
        "if len(INSTANCE_DIR) == 0: \n",
        "  INSTANCE_DIR = f'/content/training_images/{TOKEN_WORD}' \n",
        "else:\n",
        "  if INSTANCE_DIR.startswith('/content/')==False:\n",
        "   INSTANCE_DIR = '/content/' + f'{INSTANCE_DIR}'\n",
        "\n",
        "# =========================================================================================\n",
        "# CLASS IMAGE DIRECTORY\n",
        "# =========================================================================================\n",
        "CLASS_DIR =  '/content/class_images' #@param {type:\"string\"}\n",
        "if len(CLASS_DIR) == 0: \n",
        "  CLASS_DIR = f'/content/class_images/{CLASS_WORD}'\n",
        "else:\n",
        "  if CLASS_DIR.startswith('/content/')==False:\n",
        "   CLASS_DIR = '/content/' + f'{CLASS_DIR}'\n",
        "\n",
        "# =========================================================================================\n",
        "# SAVE OUTPUT DIRECTORY TO GOOGLE DRIVE?\n",
        "# =========================================================================================\n",
        "from google.colab import drive\n",
        "from os import path\n",
        "\n",
        "google_drive_dir = '/content/drive'\n",
        "\n",
        "SAVE_TO_GOOGLE_DRIVE = False #@param {type:\"boolean\"}\n",
        "\n",
        "if SAVE_TO_GOOGLE_DRIVE==True:\n",
        "  if path.exists(google_drive_dir)==False: \n",
        "    drive.mount('google_drive_dir')\n",
        "    print(f'Google Drive mounted to {google_drive_dir}')\n",
        "  else:\n",
        "    print(f'Google Drive already mounted at {google_drive_dir}')\n",
        "    \n",
        "# =========================================================================================\n",
        "# OUTPUT DIR PATH\n",
        "# =========================================================================================\n",
        "OUTPUT_DIR = \"stable_diffusion_weights/zwx\" #@param {type:\"string\"}\n",
        "\n",
        "if SAVE_TO_GOOGLE_DRIVE:\n",
        "  if len(OUTPUT_DIR)==0:\n",
        "    OUTPUT_DIR = f'{google_drive_dir}' + \"/MyDrive/\" + f'stable_diffusion_weights/{TOKEN_WORD}'\n",
        "  else:\n",
        "    OUTPUT_DIR = f'{google_drive_dir}' + \"/MyDrive/\" + OUTPUT_DIR\n",
        "else:\n",
        "  if len(OUTPUT_DIR)==0:\n",
        "    OUTPUT_DIR = \"/content/\" + f'stable_diffusion_weights/{TOKEN_WORD}'\n",
        "  else:\n",
        "    if OUTPUT_DIR.startswith('/content/') == False:\n",
        "      OUTPUT_DIR = '/content/' + f'{OUTPUT_DIR}'    \n",
        "\n",
        "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
        "\n",
        "!mkdir -p $OUTPUT_DIR\n",
        "\n",
        "# =========================================================================================\n",
        "# CONCEPTS LIST\n",
        "# =========================================================================================\n",
        "# variables used so far are for one concept / subject only\n",
        "# they are not essential as you can just type in literal strings as \n",
        "# shown in the commented-out concepts below\n",
        "# if you choose to do multi-concepts, you must create the directories manually \n",
        "# and enter the correct paths\n",
        "\n",
        "INSTANCE_PROMPT = f'{TOKEN_WORD} {CLASS_WORD}'\n",
        "#INSTANCE_PROMPT2 = f'{TOKEN_WORD2} {CLASS_WORD2}'\n",
        "#INSTANCE_PROMPT3 = f'{TOKEN_WORD3} {CLASS_WORD3}'\n",
        "\n",
        "# You can also add multiple concepts here. \n",
        "# Try tweaking `--max_train_steps` accordingly the more concepts you have.\n",
        "concepts_list = [\n",
        "    {\n",
        "        \"instance_prompt\":      f'{INSTANCE_PROMPT}',\n",
        "        \"class_prompt\":         f'{CLASS_PROMPT}',\n",
        "        \"instance_data_dir\":    f'{INSTANCE_DIR}',\n",
        "        \"class_data_dir\":       f'{CLASS_DIR}'\n",
        "    },\n",
        "#    {\n",
        "#        \"instance_prompt\":      \"skf person\",\n",
        "#        \"class_prompt\":         \"person\",\n",
        "#        \"instance_data_dir\":    \"/content/training_images/skf\",\n",
        "#        \"class_data_dir\":       \"/content/drive/MyDrive/class_images/SD1-5/person-ddim\"\n",
        "#    },\n",
        "#     {\n",
        "#         \"instance_prompt\":      \"ukj dog\",\n",
        "#         \"class_prompt\":         \"dog\",\n",
        "#         \"instance_data_dir\":    \"/content/training_images/ukj\",\n",
        "#         \"class_data_dir\":       \"/content/drive/MyDrive/class_images/SD1-5/dog-ddim\"\n",
        "#     }\n",
        "]\n",
        "\n",
        "import json\n",
        "import os\n",
        "# create an instance directory for each concept's training images\n",
        "for c in concepts_list:\n",
        "    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
        "\n",
        "# create the concepts_list.json file\n",
        "with open(\"concepts_list.json\", \"w\") as f:\n",
        "    json.dump(concepts_list, f, indent=4)\n",
        "\n",
        "# =========================================================================================\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#@markdown > **INSTANCE_DIR**: Directory for instance (training) images. Leave blank for default. \\\n",
        "#@markdown > Default is `training_images/{TOKEN_WORD}` \\\n",
        "\n",
        "#@markdown > **CLASS_DIR**: Directory for class images. Leave blank for default. \\\n",
        "#@markdown > Default is `class_images/{CLASS_WORD}` \\\n",
        "#@markdown > When training starts, if no class images exist in this directory then they will be created then (slower). \\\n",
        "\n",
        "\n",
        "#@markdown > **SAVE_TO_GOOGLE_DRIVE**: Save trained models directly to google drive. \\\n",
        "#@markdown > If you are saving multiple models at intervals as well as converting to ckpts or safetensors, you will need a lot of storage, so this is off by default.\n",
        "#@markdown > You can selectively save your model(s) to Google Drive after training.\n",
        "\n",
        "#@markdown > **OUTPUT_DIR**: Enter the directory to save trained model(s)okay in. Leave empty for default. \\\n",
        "#@markdown > _Default is : \\\n",
        "#@markdown > `stable_diffusion_weights/{TOKEN_WORD}` \\\n",
        "#@markdown > If you are saving to Google Drive then it will be: \\\n",
        "#@markdown > `drive/MyDrive/stable_diffusion_weights/{TOKEN_WORD}`_\n",
        "#@markdown ___\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FaeSRXuG6o5B"
      },
      "outputs": [],
      "source": [
        "#@title 5. Upload Instance (Training) Images ðŸŒŒðŸŒ„ðŸžï¸\n",
        "\n",
        "#@markdown If the instance directory defined in [Cell 4](#scrollTo=-APTAioh6o5A) does not already contain images, manually upload instance images to the folder. \\ \n",
        "#@markdown You can use the file manager on the left panel to upload \\\n",
        "print(\"drag and drop your images into the folder : \" + f'{INSTANCE_DIR}')\n",
        "\n",
        "#@markdown ___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn5ILIyDJIcX"
      },
      "source": [
        "Training Parameter Combinations\n",
        "\n",
        "Use the table below to choose the best flags based on your memory and speed requirements. Tested on Tesla T4 GPU.\n",
        "\n",
        "\n",
        "| `fp16` | `train_batch_size` | `gradient_accumulation_steps` | `gradient_checkpointing` | `use_8bit_adam` | GB VRAM usage | Speed (it/s) |\n",
        "| ---- | ------------------ | ----------------------------- | ----------------------- | --------------- | ---------- | ------------ |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | TRUE            | 9.92       | 0.93         |\n",
        "| no   | 1                  | 1                             | TRUE                    | TRUE            | 10.08      | 0.42         |\n",
        "| fp16 | 2                  | 1                             | TRUE                    | TRUE            | 10.4       | 0.66         |\n",
        "| fp16 | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 1.14         |\n",
        "| no   | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 0.49         |\n",
        "| fp16 | 1                  | 2                             | TRUE                    | TRUE            | 11.56      | 1            |\n",
        "| fp16 | 2                  | 1                             | FALSE                   | TRUE            | 13.67      | 0.82         |\n",
        "| fp16 | 1                  | 2                             | FALSE                   | TRUE            | 13.7       | 0.83          |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | FALSE           | 15.79      | 0.77         |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ioxxvHoicPs"
      },
      "source": [
        "Add `--gradient_checkpointing` flag for around 9.92 GB VRAM usage.\n",
        "\n",
        "remove `--use_8bit_adam` flag for full precision. Requires 15.79 GB with `--gradient_checkpointing` else 17.8 GB.\n",
        "\n",
        "remove `--train_text_encoder` flag to reduce memory usage further, degrades output quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjcSXTp-u-Eg"
      },
      "outputs": [],
      "source": [
        "#@title 6. [Training!](https://github.com/yushan777/dbsd-dec-2022/blob/main/docs/dbsd-dec-2022-readme.md#cell-6-training)\n",
        "\n",
        "NUM_CLASS_IMAGES = 200 #@param {type:\"integer\"}\n",
        "MAX_TRAIN_STEPS = 2000 #@param {type:\"integer\"}\n",
        "SAVE_INTERVAL = 500 #@param {type:\"integer\"}\n",
        "SAVE_MIN_STEPS = 500 #@param {type:\"integer\"}\n",
        "SAMPLE_PROMPT = f'a photo of {TOKEN_WORD} {CLASS_WORD}'\n",
        "\n",
        "!accelerate launch train_dreambooth.py \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --revision=\"fp16\" \\\n",
        "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "  --seed=1337 \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --train_text_encoder \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --learning_rate=1e-6 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --num_class_images=\"$NUM_CLASS_IMAGES\" \\\n",
        "  --sample_batch_size=4 \\\n",
        "  --max_train_steps=\"$MAX_TRAIN_STEPS\" \\\n",
        "  --save_interval=\"$SAVE_INTERVAL\" \\\n",
        "  --save_min_steps=\"$SAVE_MIN_STEPS\" \\\n",
        "  --save_sample_prompt=\"$SAMPLE_PROMPT\" \\\n",
        "  --concepts_list=\"concepts_list.json\"\n",
        "\n",
        "# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n",
        "# `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples directory).\n",
        "\n",
        "#@markdown > **NUM_CLASS_IMAGES**: Number of Class images to generate and/or use. \\\n",
        "#@markdown > **MAX_TRAIN_STEPS**: Maximum number of steps to train. \\\n",
        "#@markdown > **SAVE_INTERVAL**: Save weights at every N steps. (set this to same as or greater than MAX_TRAIN_STEPS to only have one set of weights saved)\\ \n",
        "#@markdown > **SAVE_MIN_STEPS**: Start saving weights at and after N steps. \\\n",
        "\n",
        "#@markdown ___\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4OY0WIGqeIbo"
      },
      "outputs": [],
      "source": [
        "#@title 7. Generate Grid of Preview Images Generated During Training (Optional)\n",
        "#@markdown Run to generate a grid of preview images from all saved weights.\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "weights_folder = OUTPUT_DIR\n",
        "folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\"], key=lambda x: int(x))\n",
        "\n",
        "row = len(folders)\n",
        "col = len(os.listdir(os.path.join(weights_folder, folders[0], \"samples\")))\n",
        "scale = 4\n",
        "fig, axes = plt.subplots(row, col, figsize=(col*scale, row*scale), gridspec_kw={'hspace': 0, 'wspace': 0})\n",
        "\n",
        "for i, folder in enumerate(folders):\n",
        "    folder_path = os.path.join(weights_folder, folder)\n",
        "    image_folder = os.path.join(folder_path, \"samples\")\n",
        "    images = [f for f in os.listdir(image_folder)]\n",
        "    for j, image in enumerate(images):\n",
        "        if row == 1:\n",
        "            currAxes = axes[j]\n",
        "        else:\n",
        "            currAxes = axes[i, j]\n",
        "        if i == 0:\n",
        "            currAxes.set_title(f\"Image {j}\")\n",
        "        if j == 0:\n",
        "            currAxes.text(-0.1, 0.5, folder, rotation=0, va='center', ha='center', transform=currAxes.transAxes)\n",
        "        image_path = os.path.join(image_folder, image)\n",
        "        img = mpimg.imread(image_path)\n",
        "        currAxes.imshow(img, cmap='gray')\n",
        "        currAxes.axis('off')\n",
        "        \n",
        "plt.tight_layout()\n",
        "plt.savefig('grid.png', dpi=72)\n",
        "\n",
        "#@markdown ___\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uqblq1nkKB-0"
      },
      "outputs": [],
      "source": [
        "#@title 8a. [Convert All Weights To ckpt / safetensors.](https://github.com/yushan777/dbsd-dec-2022/blob/main/docs/dbsd-dec-2022-readme.md#cell-8a-convert-all-weights-to-ckpt--safetensors) (To convert just one weight use cell 8b.)\n",
        "import os\n",
        "from glob import glob\n",
        "from natsort import natsorted\n",
        "# from os.path import exists\n",
        "\n",
        "MODEL_NAME_PREFIX = \"tom_zwx\" #@param {type:\"string\"}\n",
        "MODEL_FORMAT = \"ckpt\" #@param [\"ckpt\", \"safetensors\"] {type:\"string\"}\n",
        "\n",
        "use_safetensors = \"\"\n",
        "if MODEL_FORMAT == 'safetensors': \n",
        "  use_safetensors = \"--use_safetensors\"\n",
        "\n",
        "half_arg = \"\"\n",
        "#@markdown  Whether to convert to fp16, (reduces filesize down to 2GB).\n",
        "fp16 = True #@param {type: \"boolean\"}\n",
        "if fp16: \n",
        "  half_arg = \"--half\"\n",
        "\n",
        "# check dir\n",
        "#print(OUTPUT_DIR)\n",
        "\n",
        "search_pattern = OUTPUT_DIR + '/*/'\n",
        "folder_list = natsorted(glob(f'{search_pattern}', recursive=False))\n",
        "#print(len(folder_list))\n",
        "\n",
        "for folderpath in folder_list:\n",
        "    # get the last part of the path\n",
        "    step_val = os.path.basename(os.path.normpath(folderpath))\n",
        "    print(\"folderpath = \" + folderpath)\n",
        "    #print(step_val)\n",
        "    if int(step_val) > 0:\n",
        "      ckpt_path = folderpath + f'{MODEL_NAME_PREFIX}_' + f'{step_val}'\n",
        "      if MODEL_FORMAT == 'ckpt':\n",
        "        ckpt_path += '.ckpt'\n",
        "      else:\n",
        "        ckpt_path += '.safetensors'\n",
        "\n",
        "      !python convert_diffusers_to_original_stable_diffusion.py --model_path $folderpath --checkpoint_path $ckpt_path $half_arg $use_safetensors\n",
        "      \n",
        "\n",
        "      print(\"checkpoint saved : \" + ckpt_path)\n",
        "\n",
        "print(\"Complete.\")\n",
        "print(\"You can now move the checkpoints to your google drive or download them directly.\")\n",
        "\n",
        "#@markdown ___\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dcXzsUyG1aCy"
      },
      "outputs": [],
      "source": [
        "#@title 8b. [Convert Specific Weight To ckpt / safetensors](https://github.com/yushan777/dbsd-dec-2022/blob/main/docs/dbsd-dec-2022-readme.md#cell-8b-convert-specific-weight-to-ckpt--safetensors)\n",
        "import os\n",
        "\n",
        "#@markdown Specify the weights directory to use (leave blank for last saved weight.\n",
        "WEIGHTS_DIR = \"/content/stable_diffusion_weights/zwx/800/\" #@param {type:\"string\"}\n",
        "if WEIGHTS_DIR == \"\":\n",
        "    from natsort import natsorted\n",
        "    from glob import glob\n",
        "    import os\n",
        "    WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n",
        "print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")\n",
        "\n",
        "model_name_prefix = \"tom_zwx\" #@param {type:\"string\"}\n",
        "model_format = \"safetensors\" #@param [\"ckpt\", \"safetensors\"] {type:\"string\"}\n",
        "\n",
        "use_safetensors = \"\"\n",
        "if model_format == 'safetensors': \n",
        "  use_safetensors = \"--use_safetensors\"\n",
        "\n",
        "step_val = os.path.basename(os.path.normpath(WEIGHTS_DIR))\n",
        "ckpt_path = WEIGHTS_DIR + model_name_prefix + f'_{step_val}' \n",
        "if model_format == 'ckpt':\n",
        "  ckpt_path += '.ckpt'\n",
        "else:\n",
        "  ckpt_path += '.safetensors'\n",
        "print(ckpt_path)\n",
        "half_arg = \"\"\n",
        "#@markdown  Whether to convert to fp16, takes half the space (2GB).\n",
        "fp16 = True #@param {type: \"boolean\"}\n",
        "if fp16:\n",
        "    half_arg = \"--half\"\n",
        "\n",
        "\n",
        "!python convert_diffusers_to_original_stable_diffusion.py --model_path $WEIGHTS_DIR  --checkpoint_path $ckpt_path $half_arg $use_safetensors\n",
        "print(f\"[*] Converted ckpt saved at {ckpt_path}\")\n",
        "\n",
        "#@markdown ___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zPpKtMC_mz7t"
      },
      "outputs": [],
      "source": [
        "#@title 9. List Weight Directories\n",
        "from glob import glob\n",
        "from natsort import natsorted\n",
        "search_pattern = OUTPUT_DIR + '/*/'\n",
        "folder_list = natsorted(glob(f'{search_pattern}', recursive=False))\n",
        "#print(len(folder_list))\n",
        "\n",
        "for folderpath in folder_list:\n",
        "  step_val = os.path.basename(os.path.normpath(folderpath))\n",
        "  if int(step_val) > 0:\n",
        "    print(folderpath)\n",
        "\n",
        "#@markdown ___\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "K6xoHWSsbcS3",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "#@title 10. (Optional) Inference - Image Generation\n",
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "from IPython.display import display\n",
        "\n",
        "#@markdown Specify the weights directory to use for inference. (Use Cell [10b](#scrollTo=zPpKtMC_mz7t) to show a list of directories)\n",
        "WEIGHTS_DIR = \"/content/stable_diffusion_weights/zwx/900/\" #@param {type:\"string\"}\n",
        "\n",
        "model_path = WEIGHTS_DIR             # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
        "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "g_cuda = None\n",
        "\n",
        "g_cuda = torch.Generator(device='cuda')\n",
        "seed = 52362 #@param {type:\"number\"}\n",
        "g_cuda.manual_seed(seed)\n",
        "\n",
        "\n",
        "prompt = \"photo of zwx person (be sure to replace zwx with your own token word here)\" #@param {type:\"string\"}\n",
        "negative_prompt = \"\" #@param {type:\"string\"}\n",
        "num_samples = 2 #@param {type:\"number\"}\n",
        "guidance_scale = 7 #@param {type:\"number\"}\n",
        "num_inference_steps = 24 #@param {type:\"number\"}\n",
        "height = 512 #@param {type:\"number\"}\n",
        "width = 512 #@param {type:\"number\"}\n",
        "\n",
        "with autocast(\"cuda\"), torch.inference_mode():\n",
        "    images = pipe(\n",
        "        prompt,\n",
        "        height=height,\n",
        "        width=width,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_images_per_prompt=num_samples,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        generator=g_cuda\n",
        "    ).images\n",
        "\n",
        "for img in images:\n",
        "    display(img)\n",
        "\n",
        "#@markdown ___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lJoOgLQHnC8L"
      },
      "outputs": [],
      "source": [
        "#@title (Optional) Delete diffuser and old weights and only keep the ckpt / safetensors to free up drive space.\n",
        "\n",
        "#@markdown [ ! ] Caution, Only execute if you are sure u want to delete the diffuser format weights and only use the ckpt.\n",
        "import shutil\n",
        "from glob import glob\n",
        "import os\n",
        "for f in glob(OUTPUT_DIR+os.sep+\"*\"):\n",
        "    if f != WEIGHTS_DIR:\n",
        "        shutil.rmtree(f)\n",
        "        print(\"Deleted\", f)\n",
        "for f in glob(WEIGHTS_DIR+\"/*\"):\n",
        "    if not f.endswith(\".ckpt\") or not f.endswith(\".safetensors\") or not f.endswith(\".json\"):\n",
        "        try:\n",
        "            shutil.rmtree(f)\n",
        "        except NotADirectoryError:\n",
        "            continue\n",
        "        print(\"Deleted\", f)\n",
        "\n",
        "#@markdown ___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "We2VUu06-xIz"
      },
      "outputs": [],
      "source": [
        "#@title Delete a Directory and its Contents\n",
        "\n",
        "#@markdown Deleting a folder via the file manager can't be done if the folder isn't empty. This will delete a folder and everthing below it recursively.<br><br>\n",
        "\n",
        "#@markdown Specify the directory to be deleted. (Be sure it is the correct one).  colab's root folder is `/content/` \\\n",
        "#@markdown Easiest way is to right-click on the folder and copy its path. \n",
        "\n",
        "dir = \"/content/stable_diffusion_weights/zwx\" #@param {type:\"string\"}\n",
        "\n",
        "!rm -rf \"$dir\"\n",
        "\n",
        "print(\"you may need to refresh the file manager view if the folder is still visible after deleting.\")\n",
        "\n",
        "#@markdown ___\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FfxRjPxf-YUE"
      },
      "outputs": [],
      "source": [
        "#@title Empty Trash\n",
        "\n",
        "!rm -rf ~/.local/share/Trash/\n",
        "\n",
        "#@markdown ___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jXgi8HM4c-DA"
      },
      "outputs": [],
      "source": [
        "#@title Free runtime memory\n",
        "exit()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
