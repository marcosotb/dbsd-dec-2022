{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWwK5vR24PBs"
      },
      "source": [
        "# Dreambooth Stable Diffusion - Winter 2022 Edition \n",
        "\n",
        "Updated 2022-03-08\n",
        "\n",
        "This Colab is based on Shivam Shrirao's repository and has been modified to use dependencies from late 2022 but with diffusion from revision `fbdf0a17055ffa34679cb34d986fabc1296d0785` (2023-03-02).\n",
        "\n",
        "If you prefer to use an alt layout, you can use [dbsd_dec_2022.ipynb](https://colab.research.google.com/github/yushan777/dbsd-dec-2022/blob/main/dbsd_dec_2022.ipynb)\n",
        "\n",
        "___\n",
        "https://github.com/yushan777/dbsd-xmas-edition\n",
        "\n",
        "https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth\n",
        "\n",
        "\n",
        "Join the Dreambooth Discord!\n",
        "https://discord.gg/wNNs2JNF7G \n",
        "___\n",
        "#### Instructions:\n",
        "#### Run each cell in order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU7NuMAA2drw",
        "outputId": "7eb9b063-664f-4a42-e960-728ec9608c42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tesla T4, 15109 MiB, 15109 MiB\n"
          ]
        }
      ],
      "source": [
        "#@markdown Check type of GPU and VRAM available.\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzM7j0ZSc_9c"
      },
      "source": [
        "https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLWXPZqjsZVV"
      },
      "outputs": [],
      "source": [
        "#@title Install Requirements\n",
        "# commit fbdf0a17055ffa34679cb34d986fabc1296d0785 2023-03-02\n",
        "\n",
        "print(\"Installing ShivamShrirao/Diffusers...\")\n",
        "%pip install -q git+https://github.com/ShivamShrirao/diffusers.git@fbdf0a17055ffa34679cb34d986fabc1296d0785\n",
        "%pip install torch==1.13.1+cu116 torchaudio==0.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116 | grep -v 'already satisfied'\n",
        "#%pip install -q torch==1.12.1+cu116 torchvision==0.13.1+cu116 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "%pip install omegaconf==2.3.0 # required for orig-diffusers conversion script\n",
        "%pip install pytorch-lightning==1.8.5 # required for orig-diffusers conversion script\n",
        "%pip install -U --pre triton==2.0.0.dev20221030 | grep -v 'already satisfied'\n",
        "%pip install accelerate==0.12.0 transformers==4.26.0 ftfy==6.1.1 bitsandbytes==0.35.0 gradio natsort safetensors  | grep -v 'already satisfied'\n",
        "%pip install xformers==0.0.16 | grep -v 'already satisfied'\n",
        "#%pip install xformers==0.0.13 | grep -v 'already satisfied'\n",
        "\n",
        "# get train_dreambooth.py and conversion scripts\n",
        "!wget https://github.com/ShivamShrirao/diffusers/raw/fbdf0a17055ffa34679cb34d986fabc1296d0785/examples/dreambooth/train_dreambooth.py\n",
        "!wget https://github.com/ShivamShrirao/diffusers/raw/fbdf0a17055ffa34679cb34d986fabc1296d0785/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
        "!wget https://github.com/ShivamShrirao/diffusers/raw/fbdf0a17055ffa34679cb34d986fabc1296d0785/scripts/convert_original_stable_diffusion_to_diffusers.py\n",
        "!wget https://raw.githubusercontent.com/yushan777/dbsd-dec-2022/main/notification.mp3\n",
        "# =====================================================================================================================\n",
        "# remove instances of param 'keep_fp32_wrapper=True' from file 'train_dreambooth.py'\n",
        "import fileinput\n",
        "filename = 'train_dreambooth.py'\n",
        "with fileinput.FileInput(filename, inplace=True, backup='~bak') as file:\n",
        "    for line in file:\n",
        "        print(line.replace(', keep_fp32_wrapper=True', ''), end='')\n",
        "\n",
        "\n",
        "# =====================================================================================================================\n",
        "\n",
        "print(\"Finished Installing Requirements.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Rxg0y5MBudmd"
      },
      "outputs": [],
      "source": [
        "#@title Settings Model Path\n",
        "#@markdown Name/Path of the base model\n",
        "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown If you wish to convert a ckpt or safetensors file to diffusers go to cell [Download & Convert Ckpt, Safetensors To Diffusers](#scrollTo=K4curq5G4PBz)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Token & Class Word\n",
        "token_word = \"zwx\" #@param {type:\"string\"}\n",
        "class_word = \"person\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Output Directory (where your trained model will be trained)\n",
        "#@markdown If model weights should be saved directly in google drive \\\n",
        "save_to_gdrive = False #@param {type:\"boolean\"}\n",
        "if save_to_gdrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "#@markdown It is recommended to only save to google drive AFTER training is complete otherwise you could run out of storage fast (esp. if you are saving at intervals as each weight takes around 4-5 GB + another 2GB when converting back to ckpt)\\\n",
        "OUTPUT_DIR = f\"stable_diffusion_weights/{token_word}\" #@param {type:\"string\"}\n",
        "if save_to_gdrive:\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/\" + OUTPUT_DIR\n",
        "else:\n",
        "    OUTPUT_DIR = \"/content/\" + OUTPUT_DIR\n",
        "\n",
        "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
        "!mkdir -p $OUTPUT_DIR\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn5ILIyDJIcX"
      },
      "source": [
        "### Training Parameters\n",
        "\n",
        "Use the table below to choose the best flags based on your memory and speed requirements. Tested on Tesla T4 GPU.\n",
        "\n",
        "\n",
        "| `fp16` | `train_batch_size` | `gradient_accumulation_steps` | `gradient_checkpointing` | `use_8bit_adam` | GB VRAM usage | Speed (it/s) |\n",
        "| ---- | ------------------ | ----------------------------- | ----------------------- | --------------- | ---------- | ------------ |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | TRUE            | 9.92       | 0.93         |\n",
        "| no   | 1                  | 1                             | TRUE                    | TRUE            | 10.08      | 0.42         |\n",
        "| fp16 | 2                  | 1                             | TRUE                    | TRUE            | 10.4       | 0.66         |\n",
        "| fp16 | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 1.14         |\n",
        "| no   | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 0.49         |\n",
        "| fp16 | 1                  | 2                             | TRUE                    | TRUE            | 11.56      | 1            |\n",
        "| fp16 | 2                  | 1                             | FALSE                   | TRUE            | 13.67      | 0.82         |\n",
        "| fp16 | 1                  | 2                             | FALSE                   | TRUE            | 13.7       | 0.83          |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | FALSE           | 15.79      | 0.77         |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ioxxvHoicPs"
      },
      "source": [
        "Add `--gradient_checkpointing` flag for around 9.92 GB VRAM usage.\n",
        "\n",
        "remove `--use_8bit_adam` flag for full precision. Requires 15.79 GB with `--gradient_checkpointing` else 17.8 GB.\n",
        "\n",
        "remove `--train_text_encoder` flag to reduce memory usage further, degrades output quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 5. Download Regularization Images (faster than generating them prior to training.)\n",
        "#@markdown We’ve created the following image sets\n",
        "#@markdown - `man_euler` - provided by Niko Pueringer (Corridor Digital) - euler @ 40 steps, CFG 7.5\n",
        "#@markdown - `man_unsplash` - pictures from various photographers\n",
        "#@markdown - `person_ddim`\n",
        "#@markdown - `woman_ddim` - provided by David Bielejeski - ddim @ 50 steps, CFG 10.0 <br />\n",
        "#@markdown - `artstyle` - provided by Hackmans - ddim @ 50 steps, CFG 10.0 <br />\n",
        "from IPython.display import clear_output\n",
        "\n",
        "dataset=\"person_ddim\" #@param [\"man_euler\", \"man_unsplash\", \"person_ddim\", \"woman_ddim\", \"artstyle\"]\n",
        "!git clone https://github.com/djbielejeski/Stable-Diffusion-Regularization-Images-{dataset}.git\n",
        "\n",
        "!mkdir -p regularization_images/{dataset}\n",
        "!mv -v Stable-Diffusion-Regularization-Images-{dataset}/{dataset}/*.* regularization_images/{dataset}\n",
        "\n",
        "# remove temp folder now it is empty. \n",
        "!rm -rf Stable-Diffusion-Regularization-Images-{dataset}\n",
        "\n",
        "clear_output()\n",
        "print(\"✅ \\033[92mRegularization Images downloaded.\\033[0m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vDpCxId1aCm"
      },
      "outputs": [],
      "source": [
        "#@title Saved Concepts List File\n",
        "# You can also add multiple concepts here. Try tweaking `--max_train_steps` accordingly.\n",
        "\n",
        "concepts_list = [\n",
        "    {\n",
        "        \"instance_prompt\":      f\"{token_word} {class_word}\",\n",
        "        \"class_prompt\":         f\"{class_word}\",\n",
        "        \"instance_data_dir\":    f\"/content/data/{token_word}\",\n",
        "        \"class_data_dir\":       f\"/content/data/{class_word}\"\n",
        "    },\n",
        "#     {\n",
        "#         \"instance_prompt\":      \"photo of ukj person\",\n",
        "#         \"class_prompt\":         \"photo of a person\",\n",
        "#         \"instance_data_dir\":    \"/content/data/ukj\",\n",
        "#         \"class_data_dir\":       \"/content/data/person\"\n",
        "#     }\n",
        "]\n",
        "\n",
        "# `class_data_dir` contains regularization images\n",
        "import json\n",
        "import os\n",
        "for c in concepts_list:\n",
        "    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
        "\n",
        "with open(\"concepts_list.json\", \"w\") as f:\n",
        "    json.dump(concepts_list, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "32gYIDDR1aCp"
      },
      "outputs": [],
      "source": [
        "#@markdown Upload your images by running this cell.\n",
        "\n",
        "#@markdown OR\n",
        "\n",
        "#@markdown You can use the file manager on the left panel to upload (drag and drop) to each `instance_data_dir` (it uploads faster)\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "for c in concepts_list:\n",
        "    print(f\"Uploading instance images for `{c['instance_prompt']}`\")\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        dst_path = os.path.join(c['instance_data_dir'], filename)\n",
        "        shutil.move(filename, dst_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjcSXTp-u-Eg"
      },
      "outputs": [],
      "source": [
        "#@title Training!\n",
        "\n",
        "import time\n",
        "from datetime import timedelta\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "!accelerate launch train_dreambooth.py \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --revision=\"fp16\" \\\n",
        "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "  --seed=1337 \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --train_text_encoder \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --learning_rate=1e-6 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --num_class_images=50 \\\n",
        "  --sample_batch_size=4 \\\n",
        "  --max_train_steps=800 \\\n",
        "  --save_interval=10000 \\\n",
        "  --save_sample_prompt=\"photo of zwx person\" \\\n",
        "  --concepts_list=\"concepts_list.json\"\n",
        "time_taken = time.time() - start_time\n",
        "# in HH:MM:SS\n",
        "delta = str(timedelta(seconds=time_taken))\n",
        "\n",
        "# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n",
        "# `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples directory).\n",
        "\n",
        "print(\"Training Complete.  Time Taken: \" + f'{delta}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "89Az5NUxOWdy"
      },
      "outputs": [],
      "source": [
        "#@markdown Specify the weights directory to use (leave blank for latest)\n",
        "WEIGHTS_DIR = \"\" #@param {type:\"string\"}\n",
        "if WEIGHTS_DIR == \"\":\n",
        "    from natsort import natsorted\n",
        "    from glob import glob\n",
        "    import os\n",
        "    WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n",
        "print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "c6h6VxgY4PBy"
      },
      "outputs": [],
      "source": [
        "#@markdown Run to generate a grid of preview images from the last saved weights.\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "weights_folder = OUTPUT_DIR\n",
        "folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\"], key=lambda x: int(x))\n",
        "\n",
        "row = len(folders)\n",
        "col = len(os.listdir(os.path.join(weights_folder, folders[0], \"samples\")))\n",
        "scale = 4\n",
        "fig, axes = plt.subplots(row, col, figsize=(col*scale, row*scale), gridspec_kw={'hspace': 0, 'wspace': 0})\n",
        "\n",
        "for i, folder in enumerate(folders):\n",
        "    folder_path = os.path.join(weights_folder, folder)\n",
        "    image_folder = os.path.join(folder_path, \"samples\")\n",
        "    images = [f for f in os.listdir(image_folder)]\n",
        "    for j, image in enumerate(images):\n",
        "        if row == 1:\n",
        "            currAxes = axes[j]\n",
        "        else:\n",
        "            currAxes = axes[i, j]\n",
        "        if i == 0:\n",
        "            currAxes.set_title(f\"Image {j}\")\n",
        "        if j == 0:\n",
        "            currAxes.text(-0.1, 0.5, folder, rotation=0, va='center', ha='center', transform=currAxes.transAxes)\n",
        "        image_path = os.path.join(image_folder, image)\n",
        "        img = mpimg.imread(image_path)\n",
        "        currAxes.imshow(img, cmap='gray')\n",
        "        currAxes.axis('off')\n",
        "        \n",
        "plt.tight_layout()\n",
        "plt.savefig('grid.png', dpi=72)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V8wgU0HN-Kq"
      },
      "source": [
        "## Convert weights to ckpt to use in web UIs like AUTOMATIC1111."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dcXzsUyG1aCy"
      },
      "outputs": [],
      "source": [
        "#@markdown Run conversion.\n",
        "ckpt_path = WEIGHTS_DIR + \"/model.ckpt\"\n",
        "\n",
        "half_arg = \"\"\n",
        "#@markdown  Whether to convert to fp16, takes half the space (2GB).\n",
        "fp16 = True #@param {type: \"boolean\"}\n",
        "if fp16:\n",
        "    half_arg = \"--half\"\n",
        "!python convert_diffusers_to_original_stable_diffusion.py --model_path $WEIGHTS_DIR  --checkpoint_path $ckpt_path $half_arg\n",
        "print(f\"[*] Converted ckpt saved at {ckpt_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToNG4fd_dTbF"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gW15FjffdTID"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "from IPython.display import display\n",
        "\n",
        "model_path = WEIGHTS_DIR             # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
        "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "g_cuda = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIzkltjpVO_f",
        "outputId": "1db9fcaa-2d0f-4966-dc4f-baac60cdb807"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f84dc8ed450>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@markdown Can set random seed here for reproducibility.\n",
        "g_cuda = torch.Generator(device='cuda')\n",
        "seed = 52362 #@param {type:\"number\"}\n",
        "g_cuda.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "K6xoHWSsbcS3",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "#@title Run for generating images.\n",
        "\n",
        "prompt = \"photo of zwx dog in a bucket\" #@param {type:\"string\"}\n",
        "negative_prompt = \"\" #@param {type:\"string\"}\n",
        "num_samples = 4 #@param {type:\"number\"}\n",
        "guidance_scale = 7.5 #@param {type:\"number\"}\n",
        "num_inference_steps = 24 #@param {type:\"number\"}\n",
        "height = 512 #@param {type:\"number\"}\n",
        "width = 512 #@param {type:\"number\"}\n",
        "\n",
        "with autocast(\"cuda\"), torch.inference_mode():\n",
        "    images = pipe(\n",
        "        prompt,\n",
        "        height=height,\n",
        "        width=width,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_images_per_prompt=num_samples,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        generator=g_cuda\n",
        "    ).images\n",
        "\n",
        "for img in images:\n",
        "    display(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "K4curq5G4PBz"
      },
      "outputs": [],
      "source": [
        "#@title Download & Convert Ckpt, Safetensors To Diffusers \n",
        "\n",
        "%pip install omegaconf==2.3.0 # required for orig-diffusers conversion script\n",
        "%pip install pytorch-lightning==1.8.5 # required for orig-diffusers conversion script\n",
        "\n",
        "import os\n",
        "from glob import glob\n",
        "from natsort import natsorted\n",
        "\n",
        "# =========================================================================================\n",
        "# CHECKPOINTS / SAFETENSORS\n",
        "# =========================================================================================\n",
        "\n",
        "MODEL_NAME = ''\n",
        "\n",
        "#@markdown Use this cell to directly download a checkpoint/safetensor model which will be converted. \\\n",
        "CKPT_SAFETENSOR_URL = 'https://civitai.com/api/download/models/6987?type=Model&format=SafeTensor' #@param {type:\"string\"}\n",
        "\n",
        "if len(CKPT_SAFETENSOR_URL) == 0:\n",
        "  CKPT_SAFETENSOR_URL = 'https://civitai.com/api/download/models/6987?type=Model&format=SafeTensor'\n",
        "\n",
        "googlelink_prefix = 'https://drive.google.com/file/d/'\n",
        "googlelink_suffix = '/view?usp=share_link'\n",
        "\n",
        "download_dir = '/content/downloads'\n",
        "\n",
        "if path.exists(download_dir)==False:\n",
        "    os.mkdir(download_dir)\n",
        "\n",
        "# check if CKPT_SAFETENSOR_url is a google drive share link\n",
        "if CKPT_SAFETENSOR_URL.startswith(googlelink_prefix):\n",
        "    share_id = CKPT_SAFETENSOR_URL\n",
        "    print(\"is valid google share link\")\n",
        "    share_id = share_id.replace(googlelink_prefix, '')\n",
        "    share_id = share_id.replace(googlelink_suffix, '')\n",
        "    CKPT_SAFETENSOR_URL = f'https://drive.google.com/uc?id={share_id}'\n",
        "    # gdown sucks on colab and fails for large files, we will have to use wget\n",
        "    # see https://bcrf.biochem.wisc.edu/2021/02/05/download-google-drive-files-using-wget/\n",
        "    !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=$share_id' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=$share_id\" --content-disposition -P \"$download_dir\" && rm -rf /tmp/cookies.txt    \n",
        "else:\n",
        "    #download model file into download dir\n",
        "    !wget \"$CKPT_SAFETENSOR_URL\" --content-disposition -P \"$download_dir\"\n",
        "\n",
        "# get the filename\n",
        "search_pattern = f'{download_dir}/*'\n",
        "file_list = natsorted(glob(f'{search_pattern}', recursive=False))\n",
        "\n",
        "# if file_list is not empty...\n",
        "if len(file_list)==0:\n",
        "    print(\"Error: No files downloaded.\")\n",
        "else:  \n",
        "    # get last file (should be the only file)\n",
        "    file_path = file_list[-1]  \n",
        "\n",
        "if file_path.endswith('.ckpt') or file_path.endswith('.safetensors'):    \n",
        "    # filename only\n",
        "    filename = os.path.basename(os.path.normpath(file_path))\n",
        "    # save filename without extension\n",
        "    filename_no_ext = os.path.splitext(filename)[0]\n",
        "    #print(filename_no_ext)\n",
        "    # if orig. filename ext. is safetensors then set parameter flag\n",
        "\n",
        "    from_safetensors = \"\"\n",
        "    if filename.endswith('safetensors'):\n",
        "        from_safetensors = \"--from_safetensors\"\n",
        "\n",
        "    # =========================================================================================\n",
        "    # DIFFUSERS DIRECTORY\n",
        "    # =========================================================================================\n",
        "    DIFFUSERS_DIR = \"\"\n",
        "\n",
        "    if len(DIFFUSERS_DIR)==0:\n",
        "        DIFFUSERS_DIR = f'/content/diffusers-{filename_no_ext}'\n",
        "\n",
        "    print(\"converting to diffusers... \" + DIFFUSERS_DIR)\n",
        "\n",
        "\n",
        "    # convert to diffusers \n",
        "    !python convert_original_stable_diffusion_to_diffusers.py --checkpoint_path \"$file_path\" --dump_path \"$DIFFUSERS_DIR\" $from_safetensors\n",
        "\n",
        "    # set the model name/path\n",
        "    MODEL_NAME=DIFFUSERS_DIR\n",
        "\n",
        "    print(\"Model converted to diffusers : \" + f'{DIFFUSERS_DIR}')\n",
        "else:\n",
        "    print(\"Error : File downloaded is not a ckpt or safetensors model file.\")\n",
        "\n",
        "#@markdown > Example URLs:<br>\n",
        "#@markdown > `https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt` \\\n",
        "#@markdown >`https://civitai.com/api/download/models/6987?type=Model&format=PickleTensor` \\\n",
        "#@markdown >`https://civitai.com/api/download/models/6987?type=Model&format=SafeTensor` \\\n",
        "#@markdown >`https://drive.google.com/file/d/1JEZCyW36ziz9Fn482MUG8T0_2P4FGG-/view?usp=share_link` _(google drive share link)_ \\\n",
        "#@markdown >\n",
        "\n",
        "#@markdown ___\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WMCqQ5Tcdsm2"
      },
      "outputs": [],
      "source": [
        "#@markdown Run Gradio UI for generating images.\n",
        "import gradio as gr\n",
        "\n",
        "def inference(prompt, negative_prompt, num_samples, height=512, width=512, num_inference_steps=50, guidance_scale=7.5):\n",
        "    with torch.autocast(\"cuda\"), torch.inference_mode():\n",
        "        return pipe(\n",
        "                prompt, height=int(height), width=int(width),\n",
        "                negative_prompt=negative_prompt,\n",
        "                num_images_per_prompt=int(num_samples),\n",
        "                num_inference_steps=int(num_inference_steps), guidance_scale=guidance_scale,\n",
        "                generator=g_cuda\n",
        "            ).images\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            prompt = gr.Textbox(label=\"Prompt\", value=\"photo of zwx dog in a bucket\")\n",
        "            negative_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"\")\n",
        "            run = gr.Button(value=\"Generate\")\n",
        "            with gr.Row():\n",
        "                num_samples = gr.Number(label=\"Number of Samples\", value=4)\n",
        "                guidance_scale = gr.Number(label=\"Guidance Scale\", value=7.5)\n",
        "            with gr.Row():\n",
        "                height = gr.Number(label=\"Height\", value=512)\n",
        "                width = gr.Number(label=\"Width\", value=512)\n",
        "            num_inference_steps = gr.Slider(label=\"Steps\", value=24)\n",
        "        with gr.Column():\n",
        "            gallery = gr.Gallery()\n",
        "\n",
        "    run.click(inference, inputs=[prompt, negative_prompt, num_samples, height, width, num_inference_steps, guidance_scale], outputs=gallery)\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lJoOgLQHnC8L"
      },
      "outputs": [],
      "source": [
        "#@title (Optional) Delete diffuser and old weights and only keep the ckpt to free up drive space.\n",
        "\n",
        "#@markdown [ ! ] Caution, Only execute if you are sure u want to delete the diffuser format weights and only use the ckpt.\n",
        "import shutil\n",
        "from glob import glob\n",
        "import os\n",
        "for f in glob(OUTPUT_DIR+os.sep+\"*\"):\n",
        "    if f != WEIGHTS_DIR:\n",
        "        shutil.rmtree(f)\n",
        "        print(\"Deleted\", f)\n",
        "for f in glob(WEIGHTS_DIR+\"/*\"):\n",
        "    if not f.endswith(\".ckpt\") or not f.endswith(\".json\"):\n",
        "        try:\n",
        "            shutil.rmtree(f)\n",
        "        except NotADirectoryError:\n",
        "            continue\n",
        "        print(\"Deleted\", f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXgi8HM4c-DA"
      },
      "outputs": [],
      "source": [
        "#@title Free runtime memory\n",
        "exit()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
