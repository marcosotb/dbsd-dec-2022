{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqbP_AqsemqK"
      },
      "source": [
        "# Dreambooth Stable Diffusion - ☃️🎄XMas 2022 Edition🎄☃️\n",
        "This Colab is based on Shivam Shrirao's repository and has been modified to use revision hash 47f456ea3dd3c6ba3f5cc1bcc0f69e79c787208b of that repository from 2022-12-25.\n",
        "\n",
        "https://github.com/yushan777/dbsd-xmas-edition\n",
        "\n",
        "https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth\n",
        "\n",
        "https://arxiv.org/pdf/2208.12242.pdf\n",
        "\n",
        "#### Join the Dreambooth Discord!\n",
        "https://discord.gg/wNNs2JNF7G \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructions:\n",
        "#### Run each cell in order. Read instructions or notes within them.  That's it. "
      ],
      "metadata": {
        "id": "c3Vj0dWtFWXf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU7NuMAA2drw",
        "outputId": "8d27060f-4338-4a4a-e4f4-8962c1cdf898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla T4, 15360 MiB, 15101 MiB\n"
          ]
        }
      ],
      "source": [
        "#@title 1. Check type of GPU and VRAM available.\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aLWXPZqjsZVV",
        "outputId": "30626091-8c0e-4c7a-be68-ec3a0fbd92e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uninstalling existing Pytorch...\n",
            "Found existing installation: torch 1.13.1+cu116\n",
            "Uninstalling torch-1.13.1+cu116:\n",
            "  Successfully uninstalled torch-1.13.1+cu116\n",
            "Found existing installation: torchtext 0.14.1\n",
            "Uninstalling torchtext-0.14.1:\n",
            "  Successfully uninstalled torchtext-0.14.1\n",
            "Found existing installation: torchaudio 0.13.1+cu116\n",
            "Uninstalling torchaudio-0.13.1+cu116:\n",
            "  Successfully uninstalled torchaudio-0.13.1+cu116\n",
            "Found existing installation: torchvision 0.14.1+cu116\n",
            "Uninstalling torchvision-0.14.1+cu116:\n",
            "  Successfully uninstalled torchvision-0.14.1+cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/ShivamShrirao/diffusers.git@47f456ea3dd3c6ba3f5cc1bcc0f69e79c787208b\n",
            "  Cloning https://github.com/ShivamShrirao/diffusers.git (to revision 47f456ea3dd3c6ba3f5cc1bcc0f69e79c787208b) to /tmp/pip-req-build-5njdkcmw\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ShivamShrirao/diffusers.git /tmp/pip-req-build-5njdkcmw\n",
            "  Running command git rev-parse -q --verify 'sha^47f456ea3dd3c6ba3f5cc1bcc0f69e79c787208b'\n",
            "  Running command git fetch -q https://github.com/ShivamShrirao/diffusers.git 47f456ea3dd3c6ba3f5cc1bcc0f69e79c787208b\n",
            "  Running command git checkout -q 47f456ea3dd3c6ba3f5cc1bcc0f69e79c787208b\n",
            "  Resolved https://github.com/ShivamShrirao/diffusers.git to commit 47f456ea3dd3c6ba3f5cc1bcc0f69e79c787208b\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from diffusers==0.9.0) (3.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from diffusers==0.9.0) (2.25.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from diffusers==0.9.0) (8.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from diffusers==0.9.0) (6.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from diffusers==0.9.0) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from diffusers==0.9.0) (2022.6.2)\n",
            "Collecting huggingface-hub>=0.10.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.9.0) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.9.0) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.9.0) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.9.0) (4.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->diffusers==0.9.0) (3.15.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->diffusers==0.9.0) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->diffusers==0.9.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->diffusers==0.9.0) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->diffusers==0.9.0) (2022.12.7)\n",
            "Building wheels for collected packages: diffusers\n",
            "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for diffusers: filename=diffusers-0.9.0-py3-none-any.whl size=452882 sha256=408d531ea5f62a27494242d7831f7b04bc8a5c099b6c0113374f2ef3afcf31fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/0f/4c/95f2db008e3c8eac608a4bbc3a163c44bc9b940322a3746782\n",
            "Successfully built diffusers\n",
            "Installing collected packages: huggingface-hub, diffusers\n",
            "Successfully installed diffusers-0.9.0 huggingface-hub-0.12.1\n",
            "--2023-03-02 11:43:09--  https://github.com/ShivamShrirao/diffusers/raw/47f456ea3dd3c6ba3f5cc1bcc0f69e79c787208b/examples/dreambooth/train_dreambooth.py\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ShivamShrirao/diffusers/47f456ea3dd3c6ba3f5cc1bcc0f69e79c787208b/examples/dreambooth/train_dreambooth.py [following]\n",
            "--2023-03-02 11:43:10--  https://raw.githubusercontent.com/ShivamShrirao/diffusers/47f456ea3dd3c6ba3f5cc1bcc0f69e79c787208b/examples/dreambooth/train_dreambooth.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33392 (33K) [text/plain]\n",
            "Saving to: ‘train_dreambooth.py’\n",
            "\n",
            "train_dreambooth.py 100%[===================>]  32.61K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2023-03-02 11:43:10 (9.64 MB/s) - ‘train_dreambooth.py’ saved [33392/33392]\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n",
            "Collecting torch==1.13.0+cu116\n",
            "  Downloading https://download.pytorch.org/whl/cu116/torch-1.13.0%2Bcu116-cp38-cp38-linux_x86_64.whl (1983.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m895.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.14.0+cu116\n",
            "  Downloading https://download.pytorch.org/whl/cu116/torchvision-0.14.0%2Bcu116-cp38-cp38-linux_x86_64.whl (24.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.13.0\n",
            "  Downloading https://download.pytorch.org/whl/cu116/torchaudio-0.13.0%2Bcu116-cp38-cp38-linux_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.13.0+cu116) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision==0.14.0+cu116) (2.25.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.14.0+cu116) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision==0.14.0+cu116) (1.22.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.14.0+cu116) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.14.0+cu116) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.14.0+cu116) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.14.0+cu116) (4.0.0)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "Successfully installed torch-1.13.0+cu116 torchaudio-0.13.0+cu116 torchvision-0.14.0+cu116\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.7/18.7 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.0/144.0 KB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.2/14.2 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 KB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.7/140.7 KB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.2/56.2 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#@title 2. Build Environment\n",
        "\n",
        "# uninstall existing pytorch to clean things up a bit\n",
        "print(\"Uninstalling existing Pytorch...\")\n",
        "%pip uninstall torch torchtext torchaudio torchvision --y\n",
        "\n",
        "# install diffusers from the ShivamShrirao's repo, revision : 47f456ea3dd3c6ba3f5cc1bcc0f69e79c787208b (25th Dec 2022)\n",
        "%pip install git+https://github.com/ShivamShrirao/diffusers.git@47f456ea3dd3c6ba3f5cc1bcc0f69e79c787208b\n",
        "\n",
        "# get train_dreambooth.py from revision 47f456ea3dd3c6ba3f5cc1bcc0f69e79c787208b\n",
        "!wget https://github.com/ShivamShrirao/diffusers/raw/47f456ea3dd3c6ba3f5cc1bcc0f69e79c787208b/examples/dreambooth/train_dreambooth.py\n",
        "\n",
        "# for scripts we want the latest ones so that safetensors are supported\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_original_stable_diffusion_to_diffusers.py\n",
        "\n",
        "# install requisite packages\n",
        "%pip install torch==1.13.0+cu116 torchvision==0.14.0+cu116 torchaudio==0.13.0 --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "%pip install -q -U --pre triton==2.0.0.dev20221030\n",
        "%pip install -q accelerate==0.12.0 transformers==4.23.1 ftfy bitsandbytes==0.35.0 gradio natsort safetensors\n",
        "%pip install -q https://github.com/yushan777/xformers-wheels/releases/download/xformers-0.015.dev0-py38/xformers-0.0.15.dev0-cp38-cp38-linux_x86_64.whl\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "Rxg0y5MBudmd"
      },
      "outputs": [],
      "source": [
        "#@title 3. Token, Class, Prompt\n",
        "#@markdown Enter Token and Class words.  If empty, defaults will be used.\n",
        "\n",
        "# TOKEN is a unique identifier linked to the subject that you are training\n",
        "TOKEN_WORD = \"zwx\" #@param {type:\"string\"}\n",
        "if len(TOKEN_WORD) == 0:\n",
        "  TOKEN_WORD = \"zwx\"\n",
        "\n",
        "# CLASS is a coarse class descriptor of the subject (e.g. person, man, woman, cat, dog, watch, etc.).\n",
        "CLASS_WORD = \"person\" #@param {type:\"string\"}\n",
        "if len(CLASS_WORD) == 0:\n",
        "  CLASS_WORD = \"person\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-APTAioh6o5A",
        "cellView": "form",
        "outputId": "b695ce9f-c89e-4162-f5e9-2c82c3b1a745",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Drive already mounted at /content/drive\n",
            "[*] Weights will be saved at /content/stable_diffusion_weights/zwx\n"
          ]
        }
      ],
      "source": [
        "#@title 4. Google Drive, Model Paths & Directory Settings\n",
        "from google.colab import drive\n",
        "from os import path\n",
        "\n",
        "google_drive_dir = '/content/drive'\n",
        "\n",
        "#@markdown Mount Google Drive\n",
        "mount_google_drive = True #@param {type:\"boolean\"}\n",
        "if mount_google_drive==True:\n",
        "  if path.exists(google_drive_dir)==False: \n",
        "    drive.mount(google_drive_dir)\n",
        "    print(f'1Google Drive mounted to {google_drive_dir}')\n",
        "  else: \n",
        "    print(f'Google Drive already mounted at {google_drive_dir}')\n",
        "\n",
        "#@markdown Save trained models directly in google drive (will override above setting) \\\n",
        "#@markdown If you are saving multiple models at intervals you will need a lot of storage.\n",
        "save_models_to_gdrive = False #@param {type:\"boolean\"}\n",
        "if save_models_to_gdrive==True:\n",
        "  if path.exists(google_drive_dir)==False: \n",
        "    drive.mount('google_drive_dir')\n",
        "    print(f'2Google Drive mounted to {google_drive_dir}')\n",
        "  else:\n",
        "    print(f'Google Drive already mounted at {google_drive_dir}')\n",
        "    \n",
        "#@markdown Name/Path of the initial model. (this can be a HuggingFace repo address or a local path)\n",
        "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Enter the directory name to save model in. Leave empty for default. \\\n",
        "#@markdown _Default will be `stable_diffusion_weights/{TOKEN_WORD}`_\n",
        "OUTPUT_DIR = \"stable_diffusion_weights/zwx\" #@param {type:\"string\"}\n",
        "if save_models_to_gdrive:\n",
        "  if len(OUTPUT_DIR)==0:\n",
        "    OUTPUT_DIR = f'{google_drive_dir}' + \"/MyDrive/\" + f'stable_diffusion_weights/{TOKEN_WORD}'\n",
        "  else:\n",
        "    OUTPUT_DIR = f'{google_drive_dir}' + \"/MyDrive/\" + OUTPUT_DIR\n",
        "else:\n",
        "  if len(OUTPUT_DIR)==0:\n",
        "    OUTPUT_DIR = \"/content/\" + f'stable_diffusion_weights/{TOKEN_WORD}'\n",
        "  else:\n",
        "    if OUTPUT_DIR.startswith('/content/') == False:\n",
        "      OUTPUT_DIR = '/content/' + f'{OUTPUT_DIR}'    \n",
        "\n",
        "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
        "\n",
        "!mkdir -p $OUTPUT_DIR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "ARPlC5lZ6o5A"
      },
      "outputs": [],
      "source": [
        "#@title 5. Instance and Class Directory Paths\n",
        "# After running this cell, place your instance (training images) images into the directory specified here\n",
        "#@markdown #### Specify dir for instance images or leave blank for default. \\\n",
        "#@markdown _Default will be /<area>content/training_images/{TOKEN_WORD}._ \\\n",
        "INSTANCE_DIR = '/content/training_images/zwx' #@param {type:\"string\"}\n",
        "\n",
        "if len(INSTANCE_DIR) == 0: \n",
        "  INSTANCE_DIR = f'/content/training_images/{TOKEN_WORD}' \n",
        "else:\n",
        "  if INSTANCE_DIR.startswith('/content/')==False:\n",
        "   INSTANCE_DIR = '/content/' + f'{INSTANCE_DIR}'\n",
        "\n",
        "# After running this cell, place your class (regularization) images into the directory specificed here. \n",
        "# Making them readily available in Google Drive will make things faster\n",
        "\n",
        "#@markdown #### Specify dir for class images (can be prexisting directory) or leave blank for default.\n",
        "#@markdown _Default will be /<area>content/class_images/{CLASS_WORD}._ \\\n",
        "#@markdown _If no class images are found then they will be created during training (slower)._ \\\n",
        "#@markdown _If existing class images are found then they will be used. (faster)._\n",
        "CLASS_DIR =  '/content/drive/MyDrive/class_images/SD1-5/person-ddim' #@param {type:\"string\"}\n",
        "if len(CLASS_DIR) == 0: \n",
        "  CLASS_DIR = f'/content/class_images/{CLASS_WORD}'\n",
        "else:\n",
        "  if CLASS_DIR.startswith('/content/')==False:\n",
        "   CLASS_DIR = '/content/' + f'{CLASS_DIR}'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5vDpCxId1aCm"
      },
      "outputs": [],
      "source": [
        "#@title 6. Concepts List.\n",
        "# variables used so far are for one concept or subject only as well as to\n",
        "# for clarity for those who may be new to this. \n",
        "# they are not essential as you can just type in literal strings as \n",
        "# shown in the commented-out concepts below\n",
        "# if you choose to do multi-concepts, you must create the directories manually \n",
        "# and enter the correct paths\n",
        "\n",
        "INSTANCE_PROMPT = f'{TOKEN_WORD} {CLASS_WORD}'\n",
        "\n",
        "# You can also add multiple concepts here. \n",
        "# Try tweaking `--max_train_steps` accordingly the more concepts you have.\n",
        "concepts_list = [\n",
        "    {\n",
        "        \"instance_prompt\":      f'{INSTANCE_PROMPT}',\n",
        "        \"class_prompt\":         f'{CLASS_WORD}',\n",
        "        \"instance_data_dir\":    f'{INSTANCE_DIR}',\n",
        "        \"class_data_dir\":       f'{CLASS_DIR}'\n",
        "    },\n",
        "#    {\n",
        "#        \"instance_prompt\":      \"skf person\",\n",
        "#        \"class_prompt\":         \"person\",\n",
        "#        \"instance_data_dir\":    \"/content/training_images/skf\",\n",
        "#        \"class_data_dir\":       \"/content/drive/MyDrive/class_images/SD1-5/person-ddim\"\n",
        "#    },\n",
        "#     {\n",
        "#         \"instance_prompt\":      \"ukj dog\",\n",
        "#         \"class_prompt\":         \"dog\",\n",
        "#         \"instance_data_dir\":    \"/content/training_images/ukj\",\n",
        "#         \"class_data_dir\":       \"/content/drive/MyDrive/class_images/SD1-5/dog-ddim\"\n",
        "#     }\n",
        "]\n",
        "\n",
        "import json\n",
        "import os\n",
        "# create an instance directory for each concept's training images\n",
        "for c in concepts_list:\n",
        "    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
        "\n",
        "# create the concepts_list.json file\n",
        "with open(\"concepts_list.json\", \"w\") as f:\n",
        "    json.dump(concepts_list, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FaeSRXuG6o5B"
      },
      "outputs": [],
      "source": [
        "#@title 7. Upload Your Training Images 🌌🌄🏞️\n",
        "\n",
        "#@markdown You can use the file manager on the left panel to upload \\\n",
        "#@markdown (drag and drop) your instance images to `INSTANCE_DIR` defined in `CELL 5` \\ \n",
        "print(\"drag and drop your images into the folder : \" + f'{INSTANCE_DIR}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn5ILIyDJIcX"
      },
      "source": [
        "### Training Parameter Combinations\n",
        "\n",
        "Use the table below to choose the best flags based on your memory and speed requirements. Tested on Tesla T4 GPU.\n",
        "\n",
        "\n",
        "| `fp16` | `train_batch_size` | `gradient_accumulation_steps` | `gradient_checkpointing` | `use_8bit_adam` | GB VRAM usage | Speed (it/s) |\n",
        "| ---- | ------------------ | ----------------------------- | ----------------------- | --------------- | ---------- | ------------ |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | TRUE            | 9.92       | 0.93         |\n",
        "| no   | 1                  | 1                             | TRUE                    | TRUE            | 10.08      | 0.42         |\n",
        "| fp16 | 2                  | 1                             | TRUE                    | TRUE            | 10.4       | 0.66         |\n",
        "| fp16 | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 1.14         |\n",
        "| no   | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 0.49         |\n",
        "| fp16 | 1                  | 2                             | TRUE                    | TRUE            | 11.56      | 1            |\n",
        "| fp16 | 2                  | 1                             | FALSE                   | TRUE            | 13.67      | 0.82         |\n",
        "| fp16 | 1                  | 2                             | FALSE                   | TRUE            | 13.7       | 0.83          |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | FALSE           | 15.79      | 0.77         |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ioxxvHoicPs"
      },
      "source": [
        "Add `--gradient_checkpointing` flag for around 9.92 GB VRAM usage.\n",
        "\n",
        "remove `--use_8bit_adam` flag for full precision. Requires 15.79 GB with `--gradient_checkpointing` else 17.8 GB.\n",
        "\n",
        "remove `--train_text_encoder` flag to reduce memory usage further, degrades output quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jjcSXTp-u-Eg",
        "outputId": "4b71d5fb-3e67-4db3-c4f3-297229e2ea4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--num_cpu_threads_per_process` was set to `1` to improve out-of-box performance\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "Downloading (…)tokenizer/vocab.json: 100% 1.06M/1.06M [00:00<00:00, 5.88MB/s]\n",
            "Downloading (…)tokenizer/merges.txt: 100% 525k/525k [00:00<00:00, 3.48MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 472/472 [00:00<00:00, 187kB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 822/822 [00:00<00:00, 341kB/s]\n",
            "Downloading (…)_encoder/config.json: 100% 636/636 [00:00<00:00, 69.2kB/s]\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 246M/246M [00:01<00:00, 126MB/s]\n",
            "Downloading (…)_pytorch_model.bin\";: 100% 167M/167M [00:00<00:00, 176MB/s]\n",
            "Downloading (…)fp16/vae/config.json: 100% 609/609 [00:00<00:00, 229kB/s]\n",
            "Downloading (…)_pytorch_model.bin\";: 100% 1.72G/1.72G [00:09<00:00, 173MB/s]\n",
            "Downloading (…)p16/unet/config.json: 100% 806/806 [00:00<00:00, 98.8kB/s]\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n",
            "================================================================================\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/paths.py:105: UserWarning: /usr/lib64-nvidia did not contain libcudart.so as expected! Searching further paths...\n",
            "  warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-1zqrzuuoowtcy --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
            "  warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('\"172.28.0.12\",\"jupyterArgs\"'), PosixPath('6000,\"kernelManagerProxyHost\"'), PosixPath('\"/usr/local/bin/dap_multiplexer\",\"enableLsp\"'), PosixPath('{\"kernelManagerProxyPort\"'), PosixPath('[\"--ip=172.28.0.12\",\"--transport=ipc\"],\"debugAdapterMultiplexerPath\"'), PosixPath('true}')}\n",
            "  warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.8/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "/usr/local/lib/python3.8/dist-packages/diffusers/utils/deprecation_utils.py:35: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_ddpm.DDPMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
            "  warnings.warn(warning + message, FutureWarning)\n",
            "Downloading (…)cheduler_config.json: 100% 308/308 [00:00<00:00, 105kB/s]\n",
            "Caching latents: 100% 300/300 [01:11<00:00,  4.19it/s]\n",
            "2023-03-02 11:52:07.642933: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-02 11:52:11.360978: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-02 11:52:11.361215: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-02 11:52:11.361245: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Steps:  10% 200/2000 [03:04<25:57,  1.16it/s, loss=0.282, lr=1e-6]\n",
            "Downloading (…)_pytorch_model.bin\";:   0% 0.00/335M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)_pytorch_model.bin\";:   3% 10.5M/335M [00:00<00:04, 77.8MB/s]\u001b[A\n",
            "Downloading (…)_pytorch_model.bin\";:   6% 21.0M/335M [00:00<00:03, 91.3MB/s]\u001b[A\n",
            "Downloading (…)_pytorch_model.bin\";:  13% 41.9M/335M [00:00<00:02, 100MB/s] \u001b[A\n",
            "Downloading (…)_pytorch_model.bin\";:  19% 62.9M/335M [00:00<00:02, 103MB/s]\u001b[A\n",
            "Downloading (…)_pytorch_model.bin\";:  25% 83.9M/335M [00:00<00:02, 105MB/s]\u001b[A\n",
            "Downloading (…)_pytorch_model.bin\";:  31% 105M/335M [00:01<00:02, 106MB/s] \u001b[A\n",
            "Downloading (…)_pytorch_model.bin\";:  38% 126M/335M [00:01<00:01, 105MB/s]\u001b[A\n",
            "Downloading (…)_pytorch_model.bin\";:  44% 147M/335M [00:01<00:01, 107MB/s]\u001b[A\n",
            "Downloading (…)_pytorch_model.bin\";:  50% 168M/335M [00:01<00:01, 107MB/s]\u001b[A\n",
            "Downloading (…)_pytorch_model.bin\";:  56% 189M/335M [00:01<00:01, 108MB/s]\u001b[A\n",
            "Downloading (…)_pytorch_model.bin\";:  63% 210M/335M [00:01<00:01, 108MB/s]\u001b[A\n",
            "Downloading (…)_pytorch_model.bin\";:  69% 231M/335M [00:02<00:00, 108MB/s]\u001b[A\n",
            "Downloading (…)_pytorch_model.bin\";:  75% 252M/335M [00:02<00:00, 108MB/s]\u001b[A\n",
            "Downloading (…)_pytorch_model.bin\";:  81% 273M/335M [00:02<00:00, 107MB/s]\u001b[A\n",
            "Downloading (…)_pytorch_model.bin\";:  88% 294M/335M [00:02<00:00, 108MB/s]\u001b[A\n",
            "Downloading (…)_pytorch_model.bin\";:  94% 315M/335M [00:02<00:00, 108MB/s]\u001b[A\n",
            "Downloading (…)_pytorch_model.bin\";: 100% 335M/335M [00:03<00:00, 106MB/s]\n",
            "\n",
            "Downloading (…)lve/main/config.json: 100% 547/547 [00:00<00:00, 201kB/s]\n",
            "\n",
            "Downloading (…)p16/model_index.json: 100% 543/543 [00:00<00:00, 91.9kB/s]\n",
            "\n",
            "Fetching 15 files:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:   0% 0.00/608M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:   0% 0.00/246M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)rocessor_config.json: 100% 342/342 [00:00<00:00, 51.8kB/s]\n",
            "\n",
            "Fetching 15 files:   7% 1/15 [00:00<00:04,  3.38it/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)_encoder/config.json: 100% 636/636 [00:00<00:00, 91.7kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)_checker/config.json: 100% 4.70k/4.70k [00:00<00:00, 410kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)cheduler_config.json:   0% 0.00/307 [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)cial_tokens_map.json: 100% 472/472 [00:00<00:00, 56.2kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)cheduler_config.json: 100% 307/307 [00:00<00:00, 7.27kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)okenizer_config.json: 100% 822/822 [00:00<00:00, 350kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)tokenizer/merges.txt: 100% 525k/525k [00:00<00:00, 2.01MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)tokenizer/vocab.json:   0% 0.00/1.06M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)tokenizer/vocab.json: 100% 1.06M/1.06M [00:00<00:00, 2.10MB/s]\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:   4% 10.5M/246M [00:01<00:34, 6.82MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:   2% 10.5M/608M [00:01<01:29, 6.71MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:   9% 21.0M/246M [00:01<00:15, 14.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:   3% 21.0M/608M [00:01<00:41, 14.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:   5% 31.5M/608M [00:03<00:59, 9.72MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  13% 31.5M/246M [00:03<00:22, 9.63MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:   7% 41.9M/608M [00:03<00:37, 15.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  17% 41.9M/246M [00:03<00:13, 14.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  10% 62.9M/608M [00:03<00:19, 28.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  26% 62.9M/246M [00:03<00:06, 27.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  14% 83.9M/608M [00:05<00:27, 19.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  30% 73.4M/246M [00:05<00:11, 15.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  16% 94.4M/608M [00:05<00:21, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  34% 83.9M/246M [00:05<00:08, 19.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  19% 115M/608M [00:05<00:13, 35.6MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  38% 94.4M/246M [00:06<00:12, 12.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  21% 126M/608M [00:06<00:26, 18.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  22% 136M/608M [00:07<00:21, 21.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  43% 105M/246M [00:07<00:08, 15.8MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  24% 147M/608M [00:08<00:32, 14.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  47% 115M/246M [00:08<00:11, 11.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  26% 157M/608M [00:08<00:24, 18.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  51% 126M/246M [00:08<00:07, 15.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  55% 136M/246M [00:08<00:05, 20.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  60% 147M/246M [00:10<00:07, 13.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  29% 178M/608M [00:10<00:27, 15.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  64% 157M/246M [00:10<00:05, 17.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  31% 189M/608M [00:10<00:21, 19.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  68% 168M/246M [00:11<00:06, 12.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  72% 178M/246M [00:11<00:04, 16.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  81% 199M/246M [00:12<00:01, 27.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  33% 199M/608M [00:12<00:33, 12.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  89% 220M/246M [00:12<00:00, 41.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  34% 210M/608M [00:12<00:25, 15.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 246M/246M [00:12<00:00, 19.7MB/s]\n",
            "\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  36% 220M/608M [00:12<00:19, 19.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  40% 241M/608M [00:12<00:11, 32.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  43% 262M/608M [00:12<00:07, 47.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  47% 283M/608M [00:12<00:05, 63.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  52% 315M/608M [00:13<00:03, 88.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  55% 336M/608M [00:13<00:02, 103MB/s] \u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  59% 357M/608M [00:13<00:02, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  62% 377M/608M [00:13<00:01, 132MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  66% 398M/608M [00:13<00:01, 142MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  69% 419M/608M [00:13<00:01, 156MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  72% 440M/608M [00:13<00:01, 164MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  76% 461M/608M [00:13<00:00, 173MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  79% 482M/608M [00:13<00:00, 169MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  83% 503M/608M [00:14<00:00, 174MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  86% 524M/608M [00:14<00:00, 182MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  90% 545M/608M [00:14<00:00, 169MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  93% 566M/608M [00:14<00:00, 177MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";:  97% 587M/608M [00:14<00:00, 185MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 608M/608M [00:14<00:00, 41.4MB/s]\n",
            "\n",
            "Fetching 15 files: 100% 15/15 [00:14<00:00,  1.01it/s]\n",
            "/usr/local/lib/python3.8/dist-packages/diffusers/utils/deprecation_utils.py:35: FutureWarning: The configuration file of this scheduler: DDIMScheduler {\n",
            "  \"_class_name\": \"DDIMScheduler\",\n",
            "  \"_diffusers_version\": \"0.9.0\",\n",
            "  \"beta_end\": 0.012,\n",
            "  \"beta_schedule\": \"scaled_linear\",\n",
            "  \"beta_start\": 0.00085,\n",
            "  \"clip_sample\": false,\n",
            "  \"num_train_timesteps\": 1000,\n",
            "  \"prediction_type\": \"epsilon\",\n",
            "  \"set_alpha_to_one\": false,\n",
            "  \"steps_offset\": 0,\n",
            "  \"trained_betas\": null\n",
            "}\n",
            " is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\n",
            "  warnings.warn(warning + message, FutureWarning)\n",
            "\n",
            "Generating samples:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Generating samples:  25% 1/4 [00:12<00:36, 12.18s/it]\u001b[A\n",
            "Generating samples:  50% 2/4 [00:20<00:20, 10.05s/it]\u001b[A\n",
            "Generating samples:  75% 3/4 [00:29<00:09,  9.33s/it]\u001b[A\n",
            "Generating samples: 100% 4/4 [00:37<00:00,  9.43s/it]\n",
            "[*] Weights saved at /content/stable_diffusion_weights/zwx/200\n",
            "Steps:  20% 400/2000 [07:09<23:02,  1.16it/s, loss=0.263, lr=1e-6]\n",
            "Fetching 15 files: 100% 15/15 [00:00<00:00, 25348.33it/s]\n",
            "\n",
            "Generating samples:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Generating samples:  25% 1/4 [00:08<00:25,  8.54s/it]\u001b[A\n",
            "Generating samples:  50% 2/4 [00:17<00:17,  8.52s/it]\u001b[A\n",
            "Generating samples:  75% 3/4 [00:25<00:08,  8.50s/it]\u001b[A\n",
            "Generating samples: 100% 4/4 [00:34<00:00,  8.51s/it]\n",
            "[*] Weights saved at /content/stable_diffusion_weights/zwx/400\n",
            "Steps:  30% 600/2000 [10:53<20:01,  1.17it/s, loss=0.26, lr=1e-6]\n",
            "Fetching 15 files: 100% 15/15 [00:00<00:00, 116508.44it/s]\n",
            "\n",
            "Generating samples:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Generating samples:  25% 1/4 [00:08<00:25,  8.48s/it]\u001b[A\n",
            "Generating samples:  50% 2/4 [00:16<00:16,  8.48s/it]\u001b[A\n",
            "Generating samples:  75% 3/4 [00:25<00:08,  8.48s/it]\u001b[A\n",
            "Generating samples: 100% 4/4 [00:33<00:00,  8.48s/it]\n",
            "[*] Weights saved at /content/stable_diffusion_weights/zwx/600\n",
            "Steps:  40% 800/2000 [14:38<17:13,  1.16it/s, loss=0.265, lr=1e-6]\n",
            "Fetching 15 files: 100% 15/15 [00:00<00:00, 15266.82it/s]\n",
            "\n",
            "Generating samples:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Generating samples:  25% 1/4 [00:08<00:25,  8.46s/it]\u001b[A\n",
            "Generating samples:  50% 2/4 [00:16<00:16,  8.46s/it]\u001b[A\n",
            "Generating samples:  75% 3/4 [00:25<00:08,  8.46s/it]\u001b[A\n",
            "Generating samples: 100% 4/4 [00:33<00:00,  8.47s/it]\n",
            "[*] Weights saved at /content/stable_diffusion_weights/zwx/800\n",
            "Steps:  50% 1000/2000 [18:22<14:17,  1.17it/s, loss=0.264, lr=1e-6]\n",
            "Fetching 15 files: 100% 15/15 [00:00<00:00, 15980.33it/s]\n",
            "\n",
            "Generating samples:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Generating samples:  25% 1/4 [00:08<00:25,  8.58s/it]\u001b[A\n",
            "Generating samples:  50% 2/4 [00:17<00:17,  8.60s/it]\u001b[A\n",
            "Generating samples:  75% 3/4 [00:25<00:08,  8.68s/it]\u001b[A\n",
            "Generating samples: 100% 4/4 [00:34<00:00,  8.72s/it]\n",
            "[*] Weights saved at /content/stable_diffusion_weights/zwx/1000\n",
            "Steps:  60% 1200/2000 [22:08<11:24,  1.17it/s, loss=0.263, lr=1e-6]\n",
            "Fetching 15 files: 100% 15/15 [00:00<00:00, 12966.73it/s]\n",
            "\n",
            "Generating samples:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Generating samples:  25% 1/4 [00:08<00:25,  8.58s/it]\u001b[A\n",
            "Generating samples:  50% 2/4 [00:17<00:17,  8.61s/it]\u001b[A\n",
            "Generating samples:  75% 3/4 [00:26<00:08,  8.70s/it]\u001b[A\n",
            "Generating samples: 100% 4/4 [00:34<00:00,  8.74s/it]\n",
            "[*] Weights saved at /content/stable_diffusion_weights/zwx/1200\n",
            "Steps:  70% 1400/2000 [25:54<08:32,  1.17it/s, loss=0.263, lr=1e-6]\n",
            "Fetching 15 files: 100% 15/15 [00:00<00:00, 5359.90it/s]\n",
            "\n",
            "Generating samples:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Generating samples:  25% 1/4 [00:08<00:25,  8.63s/it]\u001b[A\n",
            "Generating samples:  50% 2/4 [00:17<00:17,  8.62s/it]\u001b[A\n",
            "Generating samples:  75% 3/4 [00:26<00:08,  8.70s/it]\u001b[A\n",
            "Generating samples: 100% 4/4 [00:34<00:00,  8.72s/it]\n",
            "[*] Weights saved at /content/stable_diffusion_weights/zwx/1400\n",
            "Steps:  80% 1600/2000 [29:41<05:45,  1.16it/s, loss=0.259, lr=1e-6]\n",
            "Fetching 15 files: 100% 15/15 [00:00<00:00, 27557.84it/s]\n",
            "\n",
            "Generating samples:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Generating samples:  25% 1/4 [00:08<00:25,  8.60s/it]\u001b[A\n",
            "Generating samples:  50% 2/4 [00:17<00:17,  8.62s/it]\u001b[A\n",
            "Generating samples:  75% 3/4 [00:26<00:08,  8.69s/it]\u001b[A\n",
            "Generating samples: 100% 4/4 [00:34<00:00,  8.71s/it]\n",
            "[*] Weights saved at /content/stable_diffusion_weights/zwx/1600\n",
            "Steps:  90% 1800/2000 [33:27<02:51,  1.16it/s, loss=0.26, lr=1e-6]\n",
            "Fetching 15 files: 100% 15/15 [00:00<00:00, 15827.56it/s]\n",
            "\n",
            "Generating samples:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Generating samples:  25% 1/4 [00:08<00:25,  8.55s/it]\u001b[A\n",
            "Generating samples:  50% 2/4 [00:17<00:17,  8.57s/it]\u001b[A\n",
            "Generating samples:  75% 3/4 [00:25<00:08,  8.66s/it]\u001b[A\n",
            "Generating samples: 100% 4/4 [00:34<00:00,  8.70s/it]\n",
            "[*] Weights saved at /content/stable_diffusion_weights/zwx/1800\n",
            "Steps: 100% 2000/2000 [37:11<00:00,  1.17it/s, loss=0.258, lr=1e-6]\n",
            "Fetching 15 files: 100% 15/15 [00:00<00:00, 33305.75it/s]\n",
            "\n",
            "Generating samples:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Generating samples:  25% 1/4 [00:08<00:25,  8.57s/it]\u001b[A\n",
            "Generating samples:  50% 2/4 [00:17<00:17,  8.59s/it]\u001b[A\n",
            "Generating samples:  75% 3/4 [00:25<00:08,  8.68s/it]\u001b[A\n",
            "Generating samples: 100% 4/4 [00:34<00:00,  8.71s/it]\n",
            "[*] Weights saved at /content/stable_diffusion_weights/zwx/2000\n",
            "Steps: 100% 2000/2000 [38:05<00:00,  1.14s/it, loss=0.258, lr=1e-6]\n"
          ]
        }
      ],
      "source": [
        "#@title 8. Training!\n",
        "\n",
        "SAMPLE_PROMPT = f'a photo of {TOKEN_WORD} {CLASS_WORD}'\n",
        "\n",
        "!accelerate launch train_dreambooth.py \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --revision=\"fp16\" \\\n",
        "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "  --seed=1337 \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --train_text_encoder \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --learning_rate=1e-6 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --num_class_images=300 \\\n",
        "  --sample_batch_size=4 \\\n",
        "  --max_train_steps=2000 \\\n",
        "  --save_interval=200 \\\n",
        "  --save_min_steps=200 \\\n",
        "  --save_sample_prompt=\"$SAMPLE_PROMPT\" \\\n",
        "  --concepts_list=\"concepts_list.json\"\n",
        "\n",
        "# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n",
        "# `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples directory)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4OY0WIGqeIbo"
      },
      "outputs": [],
      "source": [
        "#@title 9. Generate Grid of Preview Images Generated During Training (Optional)\n",
        "#@markdown Run to generate a grid of preview images from all saved weights.\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "weights_folder = OUTPUT_DIR\n",
        "folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\"], key=lambda x: int(x))\n",
        "\n",
        "row = len(folders)\n",
        "col = len(os.listdir(os.path.join(weights_folder, folders[0], \"samples\")))\n",
        "scale = 4\n",
        "fig, axes = plt.subplots(row, col, figsize=(col*scale, row*scale), gridspec_kw={'hspace': 0, 'wspace': 0})\n",
        "\n",
        "for i, folder in enumerate(folders):\n",
        "    folder_path = os.path.join(weights_folder, folder)\n",
        "    image_folder = os.path.join(folder_path, \"samples\")\n",
        "    images = [f for f in os.listdir(image_folder)]\n",
        "    for j, image in enumerate(images):\n",
        "        if row == 1:\n",
        "            currAxes = axes[j]\n",
        "        else:\n",
        "            currAxes = axes[i, j]\n",
        "        if i == 0:\n",
        "            currAxes.set_title(f\"Image {j}\")\n",
        "        if j == 0:\n",
        "            currAxes.text(-0.1, 0.5, folder, rotation=0, va='center', ha='center', transform=currAxes.transAxes)\n",
        "        image_path = os.path.join(image_folder, image)\n",
        "        img = mpimg.imread(image_path)\n",
        "        currAxes.imshow(img, cmap='gray')\n",
        "        currAxes.axis('off')\n",
        "        \n",
        "plt.tight_layout()\n",
        "plt.savefig('grid.png', dpi=72)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcXzsUyG1aCy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 10a. Convert specific weight to ckpt to use in web UIs like AUTOMATIC1111.\n",
        "\n",
        "#@markdown Specify the weights directory to use (leave blank for last saved weight, but you still need to run the cell)\n",
        "WEIGHTS_DIR = \"\" #@param {type:\"string\"}\n",
        "if WEIGHTS_DIR == \"\":\n",
        "    from natsort import natsorted\n",
        "    from glob import glob\n",
        "    import os\n",
        "    WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n",
        "print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")\n",
        "\n",
        "ckpt_path = WEIGHTS_DIR + \"/model.ckpt\"\n",
        "\n",
        "half_arg = \"\"\n",
        "#@markdown  Whether to convert to fp16, takes half the space (2GB).\n",
        "fp16 = True #@param {type: \"boolean\"}\n",
        "if fp16:\n",
        "    half_arg = \"--half\"\n",
        "!python convert_diffusers_to_original_stable_diffusion.py --model_path $WEIGHTS_DIR  --checkpoint_path $ckpt_path $half_arg\n",
        "print(f\"[*] Converted ckpt saved at {ckpt_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 10b Convert all weights to ckpt\n",
        "import os\n",
        "from glob import glob\n",
        "from natsort import natsorted\n",
        "from os.path import exists\n",
        "\n",
        "model_name_prefix = \"gulce\" #@param {type:\"string\"}\n",
        "\n",
        "half_arg = \"\"\n",
        "#@markdown  Whether to convert to fp16, (reduces filesize down to 2GB).\n",
        "fp16 = True #@param {type: \"boolean\"}\n",
        "if fp16: half_arg = \"--half\"\n",
        "\n",
        "# check dir\n",
        "#print(OUTPUT_DIR)\n",
        "\n",
        "search_pattern = OUTPUT_DIR + '/*/'\n",
        "folder_list = natsorted(glob(f'{search_pattern}', recursive=False))\n",
        "#print(len(folder_list))\n",
        "\n",
        "for folderpath in folder_list:\n",
        "    # get the last part of the path\n",
        "    step_val = os.path.basename(os.path.normpath(folderpath))\n",
        "    print(folderpath)\n",
        "    #print(step_val)\n",
        "    if int(step_val) > 0:\n",
        "      ckpt_path = ckpt_path = folderpath + f'{model_name_prefix}_' + f'{step_val}.ckpt'      \n",
        "      !python convert_diffusers_to_original_stable_diffusion.py --model_path $folderpath  --checkpoint_path $ckpt_path $half_arg\n",
        "      print(\"checkpoint saved : \" + ckpt_path)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uqblq1nkKB-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gW15FjffdTID"
      },
      "outputs": [],
      "source": [
        "#@title 11. Inference Setup \n",
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "from IPython.display import display\n",
        "\n",
        "model_path = WEIGHTS_DIR             # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
        "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "g_cuda = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "K6xoHWSsbcS3",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "#@title 12. Run for generating images.\n",
        "\n",
        "g_cuda = torch.Generator(device='cuda')\n",
        "seed = 52362 #@param {type:\"number\"}\n",
        "g_cuda.manual_seed(seed)\n",
        "\n",
        "\n",
        "prompt = \"photo of zwx person\" #@param {type:\"string\"}\n",
        "negative_prompt = \"\" #@param {type:\"string\"}\n",
        "num_samples = 4 #@param {type:\"number\"}\n",
        "guidance_scale = 7 #@param {type:\"number\"}\n",
        "num_inference_steps = 24 #@param {type:\"number\"}\n",
        "height = 512 #@param {type:\"number\"}\n",
        "width = 512 #@param {type:\"number\"}\n",
        "\n",
        "with autocast(\"cuda\"), torch.inference_mode():\n",
        "    images = pipe(\n",
        "        prompt,\n",
        "        height=height,\n",
        "        width=width,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_images_per_prompt=num_samples,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        generator=g_cuda\n",
        "    ).images\n",
        "\n",
        "for img in images:\n",
        "    display(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMCqQ5Tcdsm2"
      },
      "outputs": [],
      "source": [
        "#@markdown Run Gradio UI for generating images.\n",
        "import gradio as gr\n",
        "\n",
        "def inference(prompt, negative_prompt, num_samples, height=512, width=512, num_inference_steps=50, guidance_scale=7.5):\n",
        "    with torch.autocast(\"cuda\"), torch.inference_mode():\n",
        "        return pipe(\n",
        "                prompt, height=int(height), width=int(width),\n",
        "                negative_prompt=negative_prompt,\n",
        "                num_images_per_prompt=int(num_samples),\n",
        "                num_inference_steps=int(num_inference_steps), guidance_scale=guidance_scale,\n",
        "                generator=g_cuda\n",
        "            ).images\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            prompt = gr.Textbox(label=\"Prompt\", value=\"photo of zwx dog in a bucket\")\n",
        "            negative_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"\")\n",
        "            run = gr.Button(value=\"Generate\")\n",
        "            with gr.Row():\n",
        "                num_samples = gr.Number(label=\"Number of Samples\", value=4)\n",
        "                guidance_scale = gr.Number(label=\"Guidance Scale\", value=7.5)\n",
        "            with gr.Row():\n",
        "                height = gr.Number(label=\"Height\", value=512)\n",
        "                width = gr.Number(label=\"Width\", value=512)\n",
        "            num_inference_steps = gr.Slider(label=\"Steps\", value=24)\n",
        "        with gr.Column():\n",
        "            gallery = gr.Gallery()\n",
        "\n",
        "    run.click(inference, inputs=[prompt, negative_prompt, num_samples, height, width, num_inference_steps, guidance_scale], outputs=gallery)\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lJoOgLQHnC8L"
      },
      "outputs": [],
      "source": [
        "#@title (Optional) Delete diffuser and old weights and only keep the ckpt to free up drive space.\n",
        "\n",
        "#@markdown [ ! ] Caution, Only execute if you are sure u want to delete the diffuser format weights and only use the ckpt.\n",
        "import shutil\n",
        "from glob import glob\n",
        "import os\n",
        "for f in glob(OUTPUT_DIR+os.sep+\"*\"):\n",
        "    if f != WEIGHTS_DIR:\n",
        "        shutil.rmtree(f)\n",
        "        print(\"Deleted\", f)\n",
        "for f in glob(WEIGHTS_DIR+\"/*\"):\n",
        "    if not f.endswith(\".ckpt\") or not f.endswith(\".json\"):\n",
        "        try:\n",
        "            shutil.rmtree(f)\n",
        "        except NotADirectoryError:\n",
        "            continue\n",
        "        print(\"Deleted\", f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXgi8HM4c-DA"
      },
      "outputs": [],
      "source": [
        "#@title Free runtime memory\n",
        "exit()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}