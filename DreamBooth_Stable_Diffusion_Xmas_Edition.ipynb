{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqbP_AqsemqK"
      },
      "source": [
        "# Dreambooth Stable Diffusion - â˜ƒï¸ðŸŽ„XMas 2022 EditionðŸŽ„â˜ƒï¸\n",
        "This Colab is based on Shivam Shrirao's repository and has been modified to use revision `47f456ea3dd3c6ba3f5cc1bcc0f69e79c787208b` of that repository from 2022-12-25.\n",
        "\n",
        "https://github.com/yushan777/dbsd-xmas-edition\n",
        "\n",
        "https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth\n",
        "\n",
        "\n",
        "#### Join the Dreambooth Discord!\n",
        "https://discord.gg/wNNs2JNF7G \n",
        "#### ------------------------------------------------------------------------------------------------------------\n",
        "#### Instructions:\n",
        "#### Run each cell in order. Read instructions or notes within them.  That's it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XU7NuMAA2drw"
      },
      "outputs": [],
      "source": [
        "#@title 1. Check type of GPU and VRAM available.\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!wget -q -O convert_original_stable_diffusion_to_diffusers_new.py https://github.com/ShivamShrirao/diffusers/raw/8a3f0c1f7178f4a3d5a5b21ae8c2906f473e240d/scripts/convert_original_stable_diffusion_to_diffusers.py"
      ],
      "metadata": {
        "id": "oAg-0ZNMMPgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aLWXPZqjsZVV",
        "outputId": "65525b3e-99a5-48cf-b5a0-5dbb4a6b381a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uninstalling existing Pytorch...\n",
            "Installing Diffusers...\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "--2023-03-03 13:16:00--  https://github.com/ShivamShrirao/diffusers/raw/47f456ea3dd3c6ba3f5cc1bcc0f69e79c787208b/examples/dreambooth/train_dreambooth.py\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ShivamShrirao/diffusers/47f456ea3dd3c6ba3f5cc1bcc0f69e79c787208b/examples/dreambooth/train_dreambooth.py [following]\n",
            "--2023-03-03 13:16:00--  https://raw.githubusercontent.com/ShivamShrirao/diffusers/47f456ea3dd3c6ba3f5cc1bcc0f69e79c787208b/examples/dreambooth/train_dreambooth.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33392 (33K) [text/plain]\n",
            "Saving to: â€˜train_dreambooth.pyâ€™\n",
            "\n",
            "train_dreambooth.py 100%[===================>]  32.61K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-03-03 13:16:00 (26.0 MB/s) - â€˜train_dreambooth.pyâ€™ saved [33392/33392]\n",
            "\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m839.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.7/18.7 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting omegaconf==2.3.0\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.9.*\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.0/117.0 KB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.8/dist-packages (from omegaconf==2.3.0) (6.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=3672c5de2decdec9216249944a11abe2633c5be383d292000ac6755411dc9f76\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/a3/c2/6df046c09459b73cc9bb6c4401b0be6c47048baf9a1617c485\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, omegaconf\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 omegaconf-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-lightning==1.8.5\n",
            "  Downloading pytorch_lightning-1.8.5-py3-none-any.whl (800 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m800.4/800.4 KB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.3-py3-none-any.whl (518 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m518.6/518.6 KB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.8.5) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.8.5) (1.22.4)\n",
            "Collecting lightning-utilities!=0.4.0,>=0.3.0\n",
            "  Downloading lightning_utilities-0.7.1-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.8.5) (4.5.0)\n",
            "Collecting tensorboardX>=2.2\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.5/114.5 KB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.8.5) (6.0)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.8.5) (1.13.0+cu116)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.8.5) (2023.1.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.8.5) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning==1.8.5) (2.25.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning==1.8.5) (3.8.4)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX>=2.2->pytorch-lightning==1.8.5) (3.19.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.5) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.5) (3.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.5) (22.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.5) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.5) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.5) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.5) (4.0.2)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.8.5) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.8.5) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.8.5) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.8.5) (2022.12.7)\n",
            "Installing collected packages: tensorboardX, lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.7.1 pytorch-lightning-1.8.5 tensorboardX-2.6 torchmetrics-0.11.3\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m144.0/144.0 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.2/14.2 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.7/45.7 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.9/56.9 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.5/50.5 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.5/71.5 KB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m107.0/107.0 KB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m140.7/140.7 KB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.2/56.2 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m69.6/69.6 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#@title 2. Build Environment\n",
        "\n",
        "# uninstall existing pytorch to clean things up a bit\n",
        "print(\"Uninstalling existing Pytorch...\")\n",
        "%pip -q uninstall torch torchtext torchaudio torchvision --y\n",
        "\n",
        "print(\"Installing Diffusers...\")\n",
        "# install diffusers from the ShivamShrirao's repo, revision : 47f456ea3dd3c6ba3f5cc1bcc0f69e79c787208b (25th Dec 2022)\n",
        "# install diffusers from the ShivamShrirao's repo, revision : c1f887eccd2127f81283403ebcf9e2e435c8a9da (25th Jan 2023)\n",
        "%pip -q install git+https://github.com/ShivamShrirao/diffusers.git@c1f887eccd2127f81283403ebcf9e2e435c8a9da\n",
        "\n",
        "# get train_dreambooth.py from revision 47f456ea3dd3c6ba3f5cc1bcc0f69e79c787208b\n",
        "!wget https://github.com/ShivamShrirao/diffusers/raw/c1f887eccd2127f81283403ebcf9e2e435c8a9da/examples/dreambooth/train_dreambooth.py\n",
        "\n",
        "# for scripts we want the latest ones so that safetensors are supported\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/c1f887eccd2127f81283403ebcf9e2e435c8a9da/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/c1f887eccd2127f81283403ebcf9e2e435c8a9da/scripts/convert_original_stable_diffusion_to_diffusers.py\n",
        "\n",
        "# install requisite packages\n",
        "%pip install -q torch==1.12.1+cu116 torchvision==0.13.1+cu116 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "%pip install -q -U --pre triton==2.0.0.dev20221030\n",
        "%pip install omegaconf==2.3.0 # required for orig-diffusers conversion script\n",
        "%pip install pytorch-lightning==1.8.5 # required for orig-diffusers conversion script\n",
        "%pip install -q accelerate==0.12.0 transformers==4.23.1 ftfy==6.1.1 bitsandbytes==0.35.0 gradio natsort safetensors\n",
        "%pip install -q https://github.com/yushan777/xformers-wheels/releases/download/xformers-0.015.dev0-py38/xformers-0.0.15.dev0-cp38-cp38-linux_x86_64.whl\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "Rxg0y5MBudmd"
      },
      "outputs": [],
      "source": [
        "#@title 3. Token, Class, Prompt\n",
        "#@markdown Enter Token and Class words.  If empty, defaults will be used.\n",
        "\n",
        "# TOKEN is a unique identifier linked to the subject that you are training\n",
        "TOKEN_WORD = \"zwx\" #@param {type:\"string\"}\n",
        "if len(TOKEN_WORD) == 0:\n",
        "  TOKEN_WORD = \"zwx\"\n",
        "\n",
        "# CLASS is a coarse class descriptor of the subject (e.g. person, man, woman, cat, dog, watch, etc.).\n",
        "CLASS_WORD = \"person\" #@param {type:\"string\"}\n",
        "if len(CLASS_WORD) == 0:\n",
        "  CLASS_WORD = \"person\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run only one of 4a or 4b.  if you run both, then 4b will be the model used. "
      ],
      "metadata": {
        "id": "A6D-mMfDX5wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4a. Model (HuggingFace Diffusers Format)\n",
        "\n",
        "#@markdown Name/Path of the initial model. (this can be a HuggingFace repo address or a local path (diffusers format)) \\\n",
        "#@markdown Format: username/repo-name : \\\n",
        "#@markdown Examples: \\\n",
        "#@markdown `runwayml/stable-diffusion-v1-5` \\\n",
        "#@markdown `darkstorm2150/Protogen_x3.4_Official_Release`\n",
        "MODEL_NAME = \"runwayml/stable-diffusion-v1-5 \" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "9LaHzmK2MScQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4b. Model (Direct Download HuggingFace, Civitai or other)\n",
        "import os\n",
        "from glob import glob\n",
        "from natsort import natsorted\n",
        "# right-click on the download button and copy download link to get the model ID link, e.g.\n",
        "#https://civitai.com/api/download/models/6987\n",
        "\n",
        "\n",
        "#@markdown Use this cell to directly download a checkpoint (no safetensor) model. \\\n",
        "#@markdown Formats : \\\n",
        "#@markdown HuggingFace : \\\n",
        "#@markdown `https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt` \\\n",
        "#@markdown Civitai (pickletensor (ckpt) format only): \\\n",
        "#@markdown `https://civitai.com/api/download/models/6987?type=Model&format=PickleTensor` \\\n",
        "#@markdown Google Drive Share Link: \\\n",
        "#@markdown `https://drive.google.com/file/d/1JEZCyW36ziz9Fn482MUG8T0_2P4FGG-/view?usp=share_link`\\\n",
        "#@markdown \\\n",
        "#@markdown models can be either ckpt or safetensors - will be converted to diffusers: \\\n",
        "\n",
        "URL = \"https://civitai.com/api/download/models/6987?type=Model&format=PickleTensor\" #@param {type:\"string\"}\n",
        "#download model file into tmp dir\n",
        "!wget \"$URL\" --content-disposition -P /content/tmp\n",
        "\n",
        "# get the filename\n",
        "\n",
        "search_pattern = '/content/tmp' + '/*'\n",
        "file_list = natsorted(glob(f'{search_pattern}', recursive=False))\n",
        "# get last file\n",
        "file_path = file_list[-1]\n",
        "#print(file_path)\n",
        "# filename only\n",
        "filename = os.path.basename(os.path.normpath(file_path))\n",
        "#print(filename)\n",
        "# save filename without extension\n",
        "filename_no_ext = os.path.splitext(filename)[0]\n",
        "#print(filename_no_ext)\n",
        "# if orig. filename ext. is safetensors then set parameter flag\n",
        "\n",
        "if filename.endswith('safetensors'):\n",
        "  print(\"safetensors format not supported here. \")\n",
        "#@markdown Specify the directory for the converted model (leave blank for default)\n",
        "DIFFUSERS_DIR = \"\" #@param {type:\"string\"}\n",
        "if len(DIFFUSERS_DIR)==0:\n",
        "  DIFFUSERS_DIR = f'/content/diffusers-{filename_no_ext}'\n",
        "print(DIFFUSERS_DIR)\n",
        "\n",
        "# convert to diffusers \n",
        "!python convert_original_stable_diffusion_to_diffusers.py --checkpoint_path $file_path --dump_path $DIFFUSERS_DIR\n"
      ],
      "metadata": {
        "id": "7dnnzDMENf8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37d8a997-10e6-454e-b9db-de81746b71ad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-03 13:20:58--  https://civitai.com/api/download/models/6987?type=Model&format=PickleTensor\n",
            "Resolving civitai.com (civitai.com)... 172.64.128.32, 172.64.129.32, 2606:4700:e2::ac40:8120, ...\n",
            "Connecting to civitai.com (civitai.com)|172.64.128.32|:443... connected.\n",
            "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
            "Location: https://civitai-prod-settled.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com/26957/model/realisticVisionV13.WWf9.ckpt?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=2fea663d76bd24a496545da373d610fc%2F20230303%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20230303T132059Z&X-Amz-Expires=86400&X-Amz-Signature=38a0bc7c019168d243be2d6e58286db3169d8e89cf6b2cad76e440bcb3bdf1e4&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3D%22realisticVisionV13_v13.ckpt%22&x-id=GetObject [following]\n",
            "--2023-03-03 13:20:59--  https://civitai-prod-settled.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com/26957/model/realisticVisionV13.WWf9.ckpt?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=2fea663d76bd24a496545da373d610fc%2F20230303%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20230303T132059Z&X-Amz-Expires=86400&X-Amz-Signature=38a0bc7c019168d243be2d6e58286db3169d8e89cf6b2cad76e440bcb3bdf1e4&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3D%22realisticVisionV13_v13.ckpt%22&x-id=GetObject\n",
            "Resolving civitai-prod-settled.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com (civitai-prod-settled.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com)... 104.18.9.90, 104.18.8.90, 2606:4700::6812:95a, ...\n",
            "Connecting to civitai-prod-settled.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com (civitai-prod-settled.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com)|104.18.9.90|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4265379488 (4.0G) [application/octet-stream]\n",
            "Saving to: â€˜/content/tmp/realisticVisionV13_v13.ckptâ€™\n",
            "\n",
            "realisticVisionV13_ 100%[===================>]   3.97G  65.7MB/s    in 59s     \n",
            "\n",
            "2023-03-03 13:21:58 (68.8 MB/s) - â€˜/content/tmp/realisticVisionV13_v13.ckptâ€™ saved [4265379488/4265379488]\n",
            "\n",
            "/content/diffusers-realisticVisionV13_v13\n",
            "--2023-03-03 13:22:03--  https://raw.githubusercontent.com/CompVis/stable-diffusion/main/configs/stable-diffusion/v1-inference.yaml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1873 (1.8K) [text/plain]\n",
            "Saving to: â€˜v1-inference.yamlâ€™\n",
            "\n",
            "v1-inference.yaml   100%[===================>]   1.83K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-03-03 13:22:03 (36.6 MB/s) - â€˜v1-inference.yamlâ€™ saved [1873/1873]\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"convert_original_stable_diffusion_to_diffusers.py\", line 678, in <module>\n",
            "    checkpoint = checkpoint[\"state_dict\"]\n",
            "KeyError: 'state_dict'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-APTAioh6o5A",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 5. Google Drive, Model Paths & Directory Settings\n",
        "from google.colab import drive\n",
        "from os import path\n",
        "\n",
        "google_drive_dir = '/content/drive'\n",
        "\n",
        "#@markdown Mount Google Drive\n",
        "mount_google_drive = True #@param {type:\"boolean\"}\n",
        "if mount_google_drive==True:\n",
        "  if path.exists(google_drive_dir)==False: \n",
        "    drive.mount(google_drive_dir)\n",
        "    print(f'1Google Drive mounted to {google_drive_dir}')\n",
        "  else: \n",
        "    print(f'Google Drive already mounted at {google_drive_dir}')\n",
        "\n",
        "#@markdown Save trained models directly to google drive (will override above setting) \\\n",
        "#@markdown If you are saving multiple models at intervals you will need a lot of storage.\n",
        "save_models_to_gdrive = False #@param {type:\"boolean\"}\n",
        "if save_models_to_gdrive==True:\n",
        "  if path.exists(google_drive_dir)==False: \n",
        "    drive.mount('google_drive_dir')\n",
        "    print(f'2Google Drive mounted to {google_drive_dir}')\n",
        "  else:\n",
        "    print(f'Google Drive already mounted at {google_drive_dir}')\n",
        "    \n",
        "#@markdown Name/Path of the initial model. (this can be a HuggingFace repo address or a local path)\n",
        "MODEL_NAME = \"stabilityai/stable-diffusion-2-1-base\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Enter the directory name to save model in. Leave empty for default. \\\n",
        "#@markdown _Default will be `stable_diffusion_weights/{TOKEN_WORD}`_\n",
        "OUTPUT_DIR = \"stable_diffusion_weights/zwx\" #@param {type:\"string\"}\n",
        "if save_models_to_gdrive:\n",
        "  if len(OUTPUT_DIR)==0:\n",
        "    OUTPUT_DIR = f'{google_drive_dir}' + \"/MyDrive/\" + f'stable_diffusion_weights/{TOKEN_WORD}'\n",
        "  else:\n",
        "    OUTPUT_DIR = f'{google_drive_dir}' + \"/MyDrive/\" + OUTPUT_DIR\n",
        "else:\n",
        "  if len(OUTPUT_DIR)==0:\n",
        "    OUTPUT_DIR = \"/content/\" + f'stable_diffusion_weights/{TOKEN_WORD}'\n",
        "  else:\n",
        "    if OUTPUT_DIR.startswith('/content/') == False:\n",
        "      OUTPUT_DIR = '/content/' + f'{OUTPUT_DIR}'    \n",
        "\n",
        "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
        "\n",
        "!mkdir -p $OUTPUT_DIR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ARPlC5lZ6o5A"
      },
      "outputs": [],
      "source": [
        "#@title 6. Instance and Class Directory Paths\n",
        "# After running this cell, place your instance (training images) images into the directory specified here\n",
        "#@markdown #### Specify dir for instance images or leave blank for default. \\\n",
        "#@markdown _Default will be /<area>content/training_images/{TOKEN_WORD}._ \\\n",
        "INSTANCE_DIR = '/content/training_images/zwx' #@param {type:\"string\"}\n",
        "\n",
        "if len(INSTANCE_DIR) == 0: \n",
        "  INSTANCE_DIR = f'/content/training_images/{TOKEN_WORD}' \n",
        "else:\n",
        "  if INSTANCE_DIR.startswith('/content/')==False:\n",
        "   INSTANCE_DIR = '/content/' + f'{INSTANCE_DIR}'\n",
        "\n",
        "# After running this cell, place your class (regularization) images into the directory specificed here. \n",
        "# Making them readily available in Google Drive will make things faster\n",
        "\n",
        "#@markdown #### Specify dir for class images (can be prexisting directory) or leave blank for default.\n",
        "#@markdown _Default will be /<area>content/class_images/{CLASS_WORD}._ \\\n",
        "#@markdown _If no class images are found then they will be created during training (slower)._ \\\n",
        "#@markdown _If existing class images are found then they will be used. (faster)._\n",
        "CLASS_DIR =  '/content/drive/MyDrive/class_images/SD1-5/person-ddim' #@param {type:\"string\"}\n",
        "if len(CLASS_DIR) == 0: \n",
        "  CLASS_DIR = f'/content/class_images/{CLASS_WORD}'\n",
        "else:\n",
        "  if CLASS_DIR.startswith('/content/')==False:\n",
        "   CLASS_DIR = '/content/' + f'{CLASS_DIR}'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vDpCxId1aCm"
      },
      "outputs": [],
      "source": [
        "#@title 7. Concepts List.\n",
        "# variables used so far are for one concept or subject only as well as to\n",
        "# for clarity for those who may be new to this. \n",
        "# they are not essential as you can just type in literal strings as \n",
        "# shown in the commented-out concepts below\n",
        "# if you choose to do multi-concepts, you must create the directories manually \n",
        "# and enter the correct paths\n",
        "\n",
        "INSTANCE_PROMPT = f'{TOKEN_WORD} {CLASS_WORD}'\n",
        "\n",
        "# You can also add multiple concepts here. \n",
        "# Try tweaking `--max_train_steps` accordingly the more concepts you have.\n",
        "concepts_list = [\n",
        "    {\n",
        "        \"instance_prompt\":      f'{INSTANCE_PROMPT}',\n",
        "        \"class_prompt\":         f'{CLASS_WORD}',\n",
        "        \"instance_data_dir\":    f'{INSTANCE_DIR}',\n",
        "        \"class_data_dir\":       f'{CLASS_DIR}'\n",
        "    },\n",
        "#    {\n",
        "#        \"instance_prompt\":      \"skf person\",\n",
        "#        \"class_prompt\":         \"person\",\n",
        "#        \"instance_data_dir\":    \"/content/training_images/skf\",\n",
        "#        \"class_data_dir\":       \"/content/drive/MyDrive/class_images/SD1-5/person-ddim\"\n",
        "#    },\n",
        "#     {\n",
        "#         \"instance_prompt\":      \"ukj dog\",\n",
        "#         \"class_prompt\":         \"dog\",\n",
        "#         \"instance_data_dir\":    \"/content/training_images/ukj\",\n",
        "#         \"class_data_dir\":       \"/content/drive/MyDrive/class_images/SD1-5/dog-ddim\"\n",
        "#     }\n",
        "]\n",
        "\n",
        "import json\n",
        "import os\n",
        "# create an instance directory for each concept's training images\n",
        "for c in concepts_list:\n",
        "    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
        "\n",
        "# create the concepts_list.json file\n",
        "with open(\"concepts_list.json\", \"w\") as f:\n",
        "    json.dump(concepts_list, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaeSRXuG6o5B"
      },
      "outputs": [],
      "source": [
        "#@title 8. Upload Your Training Images ðŸŒŒðŸŒ„ðŸžï¸\n",
        "\n",
        "#@markdown You can use the file manager on the left panel to upload \\\n",
        "#@markdown (drag and drop) your instance images to `INSTANCE_DIR` defined in `CELL 5` \\ \n",
        "print(\"drag and drop your images into the folder : \" + f'{INSTANCE_DIR}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn5ILIyDJIcX"
      },
      "source": [
        "Training Parameter Combinations\n",
        "\n",
        "Use the table below to choose the best flags based on your memory and speed requirements. Tested on Tesla T4 GPU.\n",
        "\n",
        "\n",
        "| `fp16` | `train_batch_size` | `gradient_accumulation_steps` | `gradient_checkpointing` | `use_8bit_adam` | GB VRAM usage | Speed (it/s) |\n",
        "| ---- | ------------------ | ----------------------------- | ----------------------- | --------------- | ---------- | ------------ |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | TRUE            | 9.92       | 0.93         |\n",
        "| no   | 1                  | 1                             | TRUE                    | TRUE            | 10.08      | 0.42         |\n",
        "| fp16 | 2                  | 1                             | TRUE                    | TRUE            | 10.4       | 0.66         |\n",
        "| fp16 | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 1.14         |\n",
        "| no   | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 0.49         |\n",
        "| fp16 | 1                  | 2                             | TRUE                    | TRUE            | 11.56      | 1            |\n",
        "| fp16 | 2                  | 1                             | FALSE                   | TRUE            | 13.67      | 0.82         |\n",
        "| fp16 | 1                  | 2                             | FALSE                   | TRUE            | 13.7       | 0.83          |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | FALSE           | 15.79      | 0.77         |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ioxxvHoicPs"
      },
      "source": [
        "Add `--gradient_checkpointing` flag for around 9.92 GB VRAM usage.\n",
        "\n",
        "remove `--use_8bit_adam` flag for full precision. Requires 15.79 GB with `--gradient_checkpointing` else 17.8 GB.\n",
        "\n",
        "remove `--train_text_encoder` flag to reduce memory usage further, degrades output quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjcSXTp-u-Eg"
      },
      "outputs": [],
      "source": [
        "#@title 9. Training!\n",
        "\n",
        "SAMPLE_PROMPT = f'a photo of {TOKEN_WORD} {CLASS_WORD}'\n",
        "\n",
        "!accelerate launch train_dreambooth.py \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --revision=\"fp16\" \\\n",
        "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "  --seed=1337 \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --train_text_encoder \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --learning_rate=1e-6 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --num_class_images=300 \\\n",
        "  --sample_batch_size=4 \\\n",
        "  --max_train_steps=1000 \\\n",
        "  --save_interval=100 \\\n",
        "  --save_min_steps=100 \\\n",
        "  --save_sample_prompt=\"$SAMPLE_PROMPT\" \\\n",
        "  --concepts_list=\"concepts_list.json\"\n",
        "\n",
        "# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n",
        "# `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples directory)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4OY0WIGqeIbo"
      },
      "outputs": [],
      "source": [
        "#@title 10. Generate Grid of Preview Images Generated During Training (Optional)\n",
        "#@markdown Run to generate a grid of preview images from all saved weights.\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "weights_folder = OUTPUT_DIR\n",
        "folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\"], key=lambda x: int(x))\n",
        "\n",
        "row = len(folders)\n",
        "col = len(os.listdir(os.path.join(weights_folder, folders[0], \"samples\")))\n",
        "scale = 4\n",
        "fig, axes = plt.subplots(row, col, figsize=(col*scale, row*scale), gridspec_kw={'hspace': 0, 'wspace': 0})\n",
        "\n",
        "for i, folder in enumerate(folders):\n",
        "    folder_path = os.path.join(weights_folder, folder)\n",
        "    image_folder = os.path.join(folder_path, \"samples\")\n",
        "    images = [f for f in os.listdir(image_folder)]\n",
        "    for j, image in enumerate(images):\n",
        "        if row == 1:\n",
        "            currAxes = axes[j]\n",
        "        else:\n",
        "            currAxes = axes[i, j]\n",
        "        if i == 0:\n",
        "            currAxes.set_title(f\"Image {j}\")\n",
        "        if j == 0:\n",
        "            currAxes.text(-0.1, 0.5, folder, rotation=0, va='center', ha='center', transform=currAxes.transAxes)\n",
        "        image_path = os.path.join(image_folder, image)\n",
        "        img = mpimg.imread(image_path)\n",
        "        currAxes.imshow(img, cmap='gray')\n",
        "        currAxes.axis('off')\n",
        "        \n",
        "plt.tight_layout()\n",
        "plt.savefig('grid.png', dpi=72)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 11a. Convert All Weights To ckpt / safetensors. (To convert just one weight use cell 11b-11c.)\n",
        "import os\n",
        "from glob import glob\n",
        "from natsort import natsorted\n",
        "# from os.path import exists\n",
        "\n",
        "model_name_prefix = \"tom_zwx\" #@param {type:\"string\"}\n",
        "model_format = \"safetensors\" #@param [\"ckpt\", \"safetensors\"] {type:\"string\"}\n",
        "\n",
        "use_safetensors = \"\"\n",
        "if model_format == 'safetensors': \n",
        "  use_safetensors = \"--use_safetensors\"\n",
        "\n",
        "half_arg = \"\"\n",
        "#@markdown  Whether to convert to fp16, (reduces filesize down to 2GB).\n",
        "fp16 = True #@param {type: \"boolean\"}\n",
        "if fp16: \n",
        "  half_arg = \"--half\"\n",
        "\n",
        "# check dir\n",
        "#print(OUTPUT_DIR)\n",
        "\n",
        "search_pattern = OUTPUT_DIR + '/*/'\n",
        "folder_list = natsorted(glob(f'{search_pattern}', recursive=False))\n",
        "#print(len(folder_list))\n",
        "\n",
        "for folderpath in folder_list:\n",
        "    # get the last part of the path\n",
        "    step_val = os.path.basename(os.path.normpath(folderpath))\n",
        "    print(\"folderpath = \" + folderpath)\n",
        "    #print(step_val)\n",
        "    if int(step_val) > 0:\n",
        "      ckpt_path = folderpath + f'{model_name_prefix}_' + f'{step_val}'\n",
        "      if model_format == 'ckpt':\n",
        "        ckpt_path += '.ckpt'\n",
        "      else:\n",
        "        ckpt_path += '.safetensors'\n",
        "\n",
        "      !python convert_diffusers_to_original_stable_diffusion.py --model_path $folderpath --checkpoint_path $ckpt_path $half_arg $use_safetensors\n",
        "      \n",
        "\n",
        "      print(\"checkpoint saved : \" + ckpt_path)\n",
        "\n",
        "print(\"Complete.\")\n",
        "print(\"You can now move the checkpoints to your google drive or download them directly.\")\n"
      ],
      "metadata": {
        "id": "uqblq1nkKB-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 11b. List Weight Directories\n",
        "from glob import glob\n",
        "from natsort import natsorted\n",
        "search_pattern = OUTPUT_DIR + '/*/'\n",
        "folder_list = natsorted(glob(f'{search_pattern}', recursive=False))\n",
        "#print(len(folder_list))\n",
        "\n",
        "for folderpath in folder_list:\n",
        "  step_val = os.path.basename(os.path.normpath(folderpath))\n",
        "  if int(step_val) > 0:\n",
        "    print(folderpath)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zPpKtMC_mz7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcXzsUyG1aCy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 11c. Convert Specific Weight To ckpt / safetensors\n",
        "import os\n",
        "\n",
        "#@markdown Specify the weights directory to use (leave blank for last saved weight.\n",
        "WEIGHTS_DIR = \"/content/stable_diffusion_weights/zwx/800/\" #@param {type:\"string\"}\n",
        "if WEIGHTS_DIR == \"\":\n",
        "    from natsort import natsorted\n",
        "    from glob import glob\n",
        "    import os\n",
        "    WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n",
        "print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")\n",
        "\n",
        "model_name_prefix = \"tom_zwx\" #@param {type:\"string\"}\n",
        "model_format = \"safetensors\" #@param [\"ckpt\", \"safetensors\"] {type:\"string\"}\n",
        "\n",
        "use_safetensors = \"\"\n",
        "if model_format == 'safetensors': \n",
        "  use_safetensors = \"--use_safetensors\"\n",
        "\n",
        "step_val = os.path.basename(os.path.normpath(WEIGHTS_DIR))\n",
        "ckpt_path = WEIGHTS_DIR + model_name_prefix + f'_{step_val}' \n",
        "if model_format == 'ckpt':\n",
        "  ckpt_path += '.ckpt'\n",
        "else:\n",
        "  ckpt_path += '.safetensors'\n",
        "print(ckpt_path)\n",
        "half_arg = \"\"\n",
        "#@markdown  Whether to convert to fp16, takes half the space (2GB).\n",
        "fp16 = True #@param {type: \"boolean\"}\n",
        "if fp16:\n",
        "    half_arg = \"--half\"\n",
        "\n",
        "\n",
        "!python convert_diffusers_to_original_stable_diffusion.py --model_path $WEIGHTS_DIR  --checkpoint_path $ckpt_path $half_arg $use_safetensors\n",
        "print(f\"[*] Converted ckpt saved at {ckpt_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "K6xoHWSsbcS3",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "#@title 12. (Optional) Inference - Image Generation\n",
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "from IPython.display import display\n",
        "\n",
        "#@markdown Specify the weights directory to use for inference. (Use Cell 11b to show a list of directories)\n",
        "WEIGHTS_DIR = \"/content/stable_diffusion_weights/zwx/900/\" #@param {type:\"string\"}\n",
        "\n",
        "model_path = WEIGHTS_DIR             # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
        "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "g_cuda = None\n",
        "\n",
        "g_cuda = torch.Generator(device='cuda')\n",
        "seed = 52362 #@param {type:\"number\"}\n",
        "g_cuda.manual_seed(seed)\n",
        "\n",
        "\n",
        "prompt = \"photo of zwx person (be sure to replace zwx with your own token word here)\" #@param {type:\"string\"}\n",
        "negative_prompt = \"\" #@param {type:\"string\"}\n",
        "num_samples = 2 #@param {type:\"number\"}\n",
        "guidance_scale = 7 #@param {type:\"number\"}\n",
        "num_inference_steps = 24 #@param {type:\"number\"}\n",
        "height = 512 #@param {type:\"number\"}\n",
        "width = 512 #@param {type:\"number\"}\n",
        "\n",
        "with autocast(\"cuda\"), torch.inference_mode():\n",
        "    images = pipe(\n",
        "        prompt,\n",
        "        height=height,\n",
        "        width=width,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_images_per_prompt=num_samples,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        generator=g_cuda\n",
        "    ).images\n",
        "\n",
        "for img in images:\n",
        "    display(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMCqQ5Tcdsm2",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title (Optional) Run Gradio UI for generating images.\n",
        "#@markdown Run Gradio UI for generating images.\n",
        "import gradio as gr\n",
        "\n",
        "def inference(prompt, negative_prompt, num_samples, height=512, width=512, num_inference_steps=50, guidance_scale=7):\n",
        "    with torch.autocast(\"cuda\"), torch.inference_mode():\n",
        "        return pipe(\n",
        "                prompt, height=int(height), width=int(width),\n",
        "                negative_prompt=negative_prompt,\n",
        "                num_images_per_prompt=int(num_samples),\n",
        "                num_inference_steps=int(num_inference_steps), guidance_scale=guidance_scale,\n",
        "                generator=g_cuda\n",
        "            ).images\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            prompt = gr.Textbox(label=\"Prompt\", value=\"photo of zwx person\")\n",
        "            negative_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"\")\n",
        "            run = gr.Button(value=\"Generate\")\n",
        "            with gr.Row():\n",
        "                num_samples = gr.Number(label=\"Number of Samples\", value=4)\n",
        "                guidance_scale = gr.Number(label=\"Guidance Scale\", value=7)\n",
        "            with gr.Row():\n",
        "                height = gr.Number(label=\"Height\", value=512)\n",
        "                width = gr.Number(label=\"Width\", value=512)\n",
        "            num_inference_steps = gr.Slider(label=\"Steps\", value=24)\n",
        "        with gr.Column():\n",
        "            gallery = gr.Gallery()\n",
        "\n",
        "    run.click(inference, inputs=[prompt, negative_prompt, num_samples, height, width, num_inference_steps, guidance_scale], outputs=gallery)\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lJoOgLQHnC8L"
      },
      "outputs": [],
      "source": [
        "#@title (Optional) Delete diffuser and old weights and only keep the ckpt / safetensors to free up drive space.\n",
        "\n",
        "#@markdown [ ! ] Caution, Only execute if you are sure u want to delete the diffuser format weights and only use the ckpt.\n",
        "import shutil\n",
        "from glob import glob\n",
        "import os\n",
        "for f in glob(OUTPUT_DIR+os.sep+\"*\"):\n",
        "    if f != WEIGHTS_DIR:\n",
        "        shutil.rmtree(f)\n",
        "        print(\"Deleted\", f)\n",
        "for f in glob(WEIGHTS_DIR+\"/*\"):\n",
        "    if not f.endswith(\".ckpt\") or not f.endswith(\".safetensors\") or not f.endswith(\".json\"):\n",
        "        try:\n",
        "            shutil.rmtree(f)\n",
        "        except NotADirectoryError:\n",
        "            continue\n",
        "        print(\"Deleted\", f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Delete a Directory and its Contents\n",
        "\n",
        "#@markdown Deleting a folder via the file manager can't be done if the folder isn't empty. This will delete a folder and everthing below it recursively.<br><br>\n",
        "\n",
        "#@markdown Specify the directory to be deleted. (Be sure it is the correct one)\n",
        "dir = \"/content/diff-test\" #@param {type:\"string\"}\n",
        "\n",
        "!rm -rf $dir"
      ],
      "metadata": {
        "cellView": "form",
        "id": "We2VUu06-xIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Empty Trash\n",
        "\n",
        "!rm -rf ~/.local/share/Trash/"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FfxRjPxf-YUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXgi8HM4c-DA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Free runtime memory\n",
        "exit()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}